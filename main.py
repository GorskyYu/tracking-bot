import os
import hmac
import hashlib
import requests
import json
import base64
import redis
import logging
import re
from urllib.parse import quote
from flask import Flask, request, jsonify
import gspread
from oauth2client.service_account import ServiceAccountCredentials
from datetime import timedelta, datetime, timezone
from google.oauth2.service_account import Credentials
from googleapiclient.discovery import build
from dateutil.parser import parse as parse_date
import openai
from openai import OpenAI
from collections import defaultdict
import threading
from typing import Optional

import io
from io import BytesIO
from PIL import Image, ImageFilter
from pyzbar.pyzbar import decode, ZBarSymbol
from pdf2image import convert_from_bytes  # æ–°å¢ï¼šå°‡ PDF é é¢è½‰ç‚ºå½±åƒä¾›æ¢ç¢¼æƒæ
from PyPDF2 import PdfReader  # æ–°å¢ï¼šè§£æ PDF æ–‡å­—å…§å®¹
import fitz  # PyMuPDF

import pytz

from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry

def _build_session(timeout=(10, 30)):  # (connect, read)
    s = requests.Session()
    retries = Retry(
        total=4, backoff_factor=0.4,
        status_forcelist=(429, 500, 502, 503, 504),
        allowed_methods=frozenset(["GET","POST"])
    )
    s.mount("https://", HTTPAdapter(max_retries=retries))
    s.mount("http://",  HTTPAdapter(max_retries=retries))
    s.request = (lambda req: (lambda *a, **k: requests.Session.request(s, *a, timeout=timeout, **k)))(s.request)
    return s

HTTP = _build_session()


# --- Async executor for background OCR jobs ---
from concurrent.futures import ThreadPoolExecutor
EXECUTOR = ThreadPoolExecutor(max_workers=int(os.getenv("OCR_WORKERS", "4")))

# Optional in-memory dedupe of LINE event/file IDs to avoid double-processing on retries
_EVENT_DEDUP = set()
_EVENT_DEDUP_LOCK = threading.Lock()


# Requires:
# pip install pymupdf pillow openai

# â”€â”€â”€ Google Sheets èªè­‰ï¼ˆç’°å¢ƒè®Šæ•¸ + lazy initï¼‰â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
import os, json, base64, gspread
from google.oauth2.service_account import Credentials

SCOPES = [
    "https://www.googleapis.com/auth/spreadsheets",
    "https://www.googleapis.com/auth/drive",
]

_gs = None  # lazy singleton, avoid authenticating at import time

# â”€â”€â”€ Client â†’ LINE Group Mapping â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
CLIENT_TO_GROUP = {
    "yumi":  os.getenv("LINE_GROUP_ID_YUMI"),
    "vicky": os.getenv("LINE_GROUP_ID_VICKY"),
}

# â”€â”€â”€ Environment Variables â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
APP_ID      = os.getenv("TE_APP_ID")          # e.g. "584"
APP_SECRET  = os.getenv("TE_SECRET")          # your TE App Secret
LINE_TOKEN  = os.getenv("LINE_TOKEN")         # Channel access token

# â”€â”€â”€ LINE & ACE/SQ è¨­å®š â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ACE_GROUP_ID     = os.getenv("LINE_GROUP_ID_ACE")
SOQUICK_GROUP_ID = os.getenv("LINE_GROUP_ID_SQ")
VICKY_GROUP_ID   = os.getenv("LINE_GROUP_ID_VICKY")
VICKY_USER_ID    = os.getenv("VICKY_USER_ID") 
YVES_USER_ID     = os.getenv("YVES_USER_ID") 
YUMI_GROUP_ID    = os.getenv("LINE_GROUP_ID_YUMI")
JOYCE_GROUP_ID   = os.getenv("LINE_GROUP_ID_JOYCE")
PDF_GROUP_ID     = os.getenv("LINE_GROUP_ID_PDF")

SQ_SHEET_URL     = os.getenv("SQ_SHEET_URL")
ACE_SHEET_URL = os.getenv("ACE_SHEET_URL")

# --- Timezone (used by schedulers) ---
TIMEZONE = os.getenv("TIMEZONE", "America/Vancouver")

# Trigger when you see â€œé€±å››å‡ºè²¨â€/â€œé€±æ—¥å‡ºè²¨â€ + â€œéº»ç…©è«‹â€ + an ACE or 250N code,
# or when you see the exact phrase â€œé€™å¹¾ä½é‚„æ²’æœ‰æŒ‰ç”³å ±ç›¸ç¬¦â€
CODE_TRIGGER_RE = re.compile(r"\b(?:ACE|250N)\d+[A-Z0-9]*\b")
MISSING_CONFIRM = "é€™å¹¾ä½é‚„æ²’æœ‰æŒ‰ç”³å ±ç›¸ç¬¦"

# Names to look for in each groupâ€™s list
VICKY_NAMES = {"é¡§å®¶çª","é¡§å¿—å¿ ","å‘¨ä½©æ¨º","é¡§éƒ­è“®æ¢…","å»–èŠ¯å„€","æ—å¯¶ç²","é«˜æ‡¿æ¬£","å´”æ›¸é³³"}
YUMI_NAMES  = {"åŠ‰æ·‘ç‡•","ç«‡æ°¸è£•","åŠ‰æ·‘ç«","åŠ‰æ·‘èŒ¹","é™³å¯Œç¾","åŠ‰ç¦ç¥¥","éƒ­æ·¨å´‘","é™³å‰æ€¡","æ´ªç‘œé§¿"}
YVES_NAMES = {
    "æ¢ç©ç¦",
    "å¼µè© å‡±",
    "åŠ‰è‚²ä¼¶",
    "ç¾…å”¯è‹±",
    "é™³å“èŒ¹",
    "å¼µç¢§è“®",
    "å³æ”¿è",
    "è§£ç‘‹åº­",
    "æ´ªå›è±ª",
    "æ´ªèŠ·ç¿",
    "ç¾…æœ¨ç™¸",
    "æ´ªé‡‘ç ",
    "æ—æ†¶æ…§",
    "è‘‰æ€¡ç§€",
    "è‘‰è©¹æ˜",
    "å»–è°æ¯…",
    "è”¡è‹±è±ª",
    "é­åª´è“",
    "é»ƒæ·‘èŠ¬",
    "è§£ä½©é ´",
    "æ›¹èŠ·èŒœ",
    "ç‹è© çš“",
    "æ›¹äº¦èŠ³",
    "ææ…§èŠ",
    "æéŒ¦ç¥¥",
    "è©¹æ¬£é™µ",
    "é™³å¿—è³¢",
    "æ›¾æƒ ç²",
    "æç™½ç§€",
    "é™³è–ç„",
    "æŸ¯é›…ç”„",
    "æ¸¸ç‰æ…§",
    "æ¸¸ç¹¼å ¯",
    "æ¸¸æ‰¿å“²",
    "æ¸¸å‚³æ°",
    "é™³ç§€è¯",
    "é™³ç§€ç²",
    "é™³æ’æ¥·"
}
EXCLUDED_SENDERS = {"Yves Lai", "Yves KT Lai", "Yves MM Lai", "Yumi Liu", "Vicky Ku"}

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ACEã€Œä»Šæ—¥å‡ºè²¨ã€ï¼šæ’ç¨‹ + æ‰‹å‹•è§¸ç™¼
# - ä¾†æºï¼šACE_SHEET_URL æŒ‡å‘çš„ Google Sheet
# - è¦å‰‡ï¼šæ‰¾å‡ºã€Œä»Šå¤©ã€åœ¨æ¬„ A çš„æ‰€æœ‰åˆ— â†’ å–è©²åˆ—çš„æ¬„ Bï¼ˆBox IDï¼‰ï¼Œçµ„æˆè¨Šæ¯æ¨æ’­
# - æ™‚é–“ï¼šæ¯é€±å››ã€é€±æ—¥ä¸‹åˆ 4:00ï¼ˆAmerica/Vancouverï¼‰
# - æ‰‹å‹•ï¼šåœ¨ ACE ç¾¤çµ„è¼¸å…¥ã€Œå·²ä¸Šå‚³è³‡æ–™å¯å‡ºè²¨ã€ç«‹å³è§¸ç™¼ï¼ˆä¸å—æ¯æ—¥é˜²é‡è¤‡é™åˆ¶ï¼‰
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

# â”€â”€â”€ Redis for state persistence â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
REDIS_URL = os.getenv("REDIS_URL")
if not REDIS_URL:
    raise RuntimeError("REDIS_URL environment variable is required for state persistence")
r = redis.from_url(REDIS_URL, decode_responses=True)

# pull your sheet URL / ID from env
VICKY_SHEET_URL = os.getenv("VICKY_SHEET_URL")

MONDAY_API_TOKEN = os.getenv("MONDAY_API_TOKEN")

AIR_BOARD_ID = os.getenv("AIR_BOARD_ID")
AIR_PARENT_BOARD_ID = os.getenv("AIR_PARENT_BOARD_ID")

#STATE_FILE = os.getenv("STATE_FILE", "last_seen.json")
LINE_PUSH_URL = "https://api.line.me/v2/bot/message/push"
LINE_HEADERS = {
    "Content-Type":  "application/json",
    "Authorization": f"Bearer {LINE_TOKEN}"
}
LINE_REPLY_URL = "https://api.line.me/v2/bot/message/reply"


from apscheduler.schedulers.background import BackgroundScheduler
import atexit

def _ace_collect_today_box_ids(sheet_url: str) -> list[str]:
    """
    è®€å–æŒ‡å®š Google Sheetï¼ˆä½¿ç”¨ service accountï¼‰ï¼Œæ‰¾å‡ºæ¬„ A ç­‰æ–¼ã€Œä»Šå¤©ã€çš„åˆ—ï¼Œ
    å›å‚³æ‰€æœ‰è©²åˆ—æ¬„ Bï¼ˆBox IDï¼‰çš„æ¸…å–®ï¼Œä½†ã€åŒåˆ—çš„æ¬„ Cï¼ˆå¯„ä»¶äººï¼‰ä¹Ÿå¿…é ˆéç©ºã€‘ã€‚

    - æ¬„ Aï¼šæ—¥æœŸï¼ˆå¯èƒ½æ˜¯é¡¯ç¤ºæ–‡å­—æˆ–åŸå§‹å€¼ï¼Œä½¿ç”¨ dateutil.parse ç›¡é‡è§£æï¼‰
    - æ¬„ Bï¼šBox ID
    - æ¬„ Cï¼šå¯„ä»¶äººï¼ˆå¿…é ˆéç©ºæ‰æ”¶é›†ï¼‰
    """
    gs = get_gspread_client()
    # é è¨­å–ç¬¬ä¸€å€‹å·¥ä½œè¡¨ï¼ˆsheet1ï¼‰ã€‚è‹¥ä½ çš„ ACE sheet ä¸æ˜¯ç¬¬ä¸€å€‹ï¼Œå¯ä»¥æ”¹é–‹ç‰¹å®š titleã€‚
    ws = gs.open_by_url(sheet_url).sheet1
    rows = ws.get_all_values()  # åŒ…å«è¡¨é ­çš„äºŒç¶­é™£åˆ—ï¼Œæ¯åˆ—çš†ç‚ºå­—ä¸²æ¸…å–®

    # ä»Šå¤©ï¼ˆä»¥ç³»çµ±è¨­å®šçš„ TIMEZONEï¼America/Vancouver ç‚ºæº–ï¼‰
    tz = pytz.timezone(TIMEZONE)
    today_local = datetime.now(tz).date()

    box_ids: list[str] = []
    # å‡è¨­ç¬¬ä¸€åˆ—æ˜¯è¡¨é ­ï¼Œå¾ç¬¬äºŒåˆ—é–‹å§‹æƒæï¼›è‹¥ç„¡è¡¨é ­å¯æ”¹æˆ enumerate(rows, start=1)
    for i, row in enumerate(rows[1:], start=2):
        # row è‡³å°‘è¦æœ‰ Aã€Bã€C ä¸‰æ¬„
        if not row or len(row) < 3:
            continue

        col_a = (row[0] or "").strip()  # æ—¥æœŸ
        col_b = (row[1] or "").strip()  # Box ID
        col_c = (row[2] or "").strip()  # å¯„ä»¶äºº

        if not col_a:
            continue

        # è§£ææ¬„ A æ—¥æœŸæ–‡å­—ï¼šå¯èƒ½æ˜¯ "2025-10-10"ã€"10/10/2025"ã€"Oct 10, 2025" ç­‰
        try:
            d = parse_date(col_a).date()
        except Exception:
            # è‹¥ç„¡æ³•è§£æï¼Œç›´æ¥ç•¥éè©²åˆ—ï¼ˆä¸æ‹‹éŒ¯ä»¥é¿å…ä¸­æ–·å…¨æµç¨‹ï¼‰
            continue

        # æ—¥æœŸæ¯”å°ï¼ˆä»¥ã€ŒåŒä¸€å¤©ã€ç‚ºæº–ï¼Œä¸å«æ™‚é–“ï¼‰
        if d == today_local and col_b and col_c:
            box_ids.append(col_b)

    # å»é‡ï¼ˆä¿ç•™é¦–æ¬¡å‡ºç¾çš„é †åºï¼‰
    seen = set()
    unique_box_ids = []
    for x in box_ids:
        if x not in seen:
            seen.add(x)
            unique_box_ids.append(x)

    return unique_box_ids


def push_ace_today_shipments(*, force: bool = False, reply_token: str | None = None):
    """
    æ¨æ’­ ACEã€Œä»Šæ—¥å‡ºè²¨ã€è¨Šæ¯åˆ° ACE ç¾¤çµ„ã€‚

    åƒæ•¸ï¼š
      - force: True æ™‚ã€Œä¸ã€ä½¿ç”¨æ¯æ—¥é˜²é‡è¤‡æ©Ÿåˆ¶ï¼ˆé©ç”¨æ‰‹å‹•æ¸¬è©¦ï¼‰
               False æ™‚å•Ÿç”¨æ¯æ—¥é˜²é‡è¤‡ï¼ˆæ’ç¨‹å‘¼å«ï¼‰
      - reply_token: è‹¥æä¾›ï¼Œæœƒå…ˆç”¨ reply API å›è¦†æ¸¬è©¦è€…ï¼Œå†é€²è¡Œæ¨æ’­ï¼ˆé«”é©—è¼ƒå³æ™‚ï¼‰

    é‚è¼¯ï¼š
      1) ï¼ˆé forceï¼‰ç”¨ Redis è¨­å®šç•¶æ—¥é˜²é‡è¤‡ keyï¼Œé¿å…é‡è¤‡æ¨é€ã€‚
      2) è®€å–ä»Šå¤©çš„ Box IDsï¼›è‹¥ç‚ºç©ºï¼Œè¨˜éŒ„ log ä¸¦ï¼ˆé forceï¼‰ä¹Ÿè¨­ guardï¼Œé¿å…é‡è¤‡å˜—è©¦ã€‚
      3) æ¨é€è¨Šæ¯æ ¼å¼ï¼š
         - æœ‰è³‡æ–™ï¼š "ä»Šæ—¥å‡ºè²¨ï¼šID1, ID2, ID3"
         - ç„¡è³‡æ–™ï¼ˆforce æ‰‹å‹•æ¸¬ï¼‰ï¼šå›è¦† "ä»Šæ—¥å‡ºè²¨ï¼šç›®å‰ç„¡è³‡æ–™ï¼ˆæ¸¬è©¦ï¼‰"
      4) ï¼ˆforceï¼‰ä¸å¯«å…¥ guardï¼›ï¼ˆé forceï¼‰æˆåŠŸå¾Œå¯«å…¥ guardï¼ˆ48h éæœŸï¼‰ã€‚
    """
    tz = pytz.timezone(TIMEZONE)
    today_str = datetime.now(tz).strftime("%Y-%m-%d")
    guard_key = f"ace_today_shipments_pushed_{today_str}"

    # æ‰‹å‹•æ¸¬è©¦æ™‚ï¼Œå…ˆå›ä¸€å‰‡çŸ­è¨Šè®“æ“ä½œè€…çŸ¥é“å·²è§¸ç™¼
    if reply_token:
        try:
            requests.post(
                LINE_REPLY_URL,
                headers=LINE_HEADERS,
                json={"replyToken": reply_token, "messages": [{"type": "text", "text": "å·²æ¥æ”¶ï¼Œæ­£åœ¨æª¢æŸ¥ä»Šæ—¥å‡ºè²¨â€¦"}]},
                timeout=10,
            )
        except Exception as e:
            log.warning(f"[ACE Today] reply (pre-ack) failed: {e}")

    # é forceï¼ˆæ’ç¨‹ï¼‰â†’ é–‹å•Ÿæ¯æ—¥é˜²é‡è¤‡
    if not force and r.get(guard_key):
        log.info("[ACE Today] Already pushed for today; skipping.")
        return

    try:
        ids = _ace_collect_today_box_ids(ACE_SHEET_URL)

        # çµ„å‡ºè¨Šæ¯
        if ids:
            base_text = "ä»Šæ—¥å‡ºè²¨ï¼š" + ", ".join(ids)
            text = base_text if not force else (base_text + "ï¼ˆæ¸¬è©¦ï¼‰")
        else:
            # æ²’æœ‰è³‡æ–™ï¼šæ’ç¨‹ï¼ˆé forceï¼‰æ™‚ä¸æ¨ç¾¤çµ„ï¼›æ‰‹å‹•ï¼ˆforceï¼‰æ™‚å›æ¸¬è©¦è¨Šæ¯
            if force:
                try:
                    # æ‰‹å‹•æ¸¬è©¦ï¼šè‹¥æœ‰ reply_token ç”¨ reply å›å‚³ï¼Œå¦å‰‡ä¹Ÿå¯ç›´æ¥ç”¨æ¨æ’­ï¼ˆä½†é¿å…æ‰“æ“¾å…¨ç¾¤ï¼‰
                    if reply_token:
                        requests.post(
                            LINE_REPLY_URL,
                            headers=LINE_HEADERS,
                            json={"replyToken": reply_token, "messages": [{"type": "text", "text": "ä»Šæ—¥å‡ºè²¨ï¼šç›®å‰ç„¡è³‡æ–™ï¼ˆæ¸¬è©¦ï¼‰"}]},
                            timeout=10,
                        )
                    else:
                        # æ²’æœ‰ reply_token æ‰ä½¿ç”¨æ¨æ’­ï¼ˆè¼ƒåµï¼‰ï¼Œä¸€èˆ¬ä¸å»ºè­°
                        requests.post(
                            LINE_PUSH_URL,
                            headers=LINE_HEADERS,
                            json={"to": ACE_GROUP_ID, "messages": [{"type": "text", "text": "ä»Šæ—¥å‡ºè²¨ï¼šç›®å‰ç„¡è³‡æ–™ï¼ˆæ¸¬è©¦ï¼‰"}]},
                            timeout=10,
                        )
                except Exception as e:
                    log.error(f"[ACE Today] Manual test (no data) notify failed: {e}")
                return
            else:
                log.info("[ACE Today] No box IDs for today; nothing to push.")
                # æ’ç¨‹æ™‚ï¼šä»ç„¶å¯«å…¥ guard é¿å…é‡è¦†æŸ¥è©¢ï¼æ¨é€
                r.set(guard_key, "1", ex=48 * 3600)
                return

        # é€å‡ºç¾¤çµ„æ¨æ’­
        payload = {"to": ACE_GROUP_ID, "messages": [{"type": "text", "text": text}]}
        resp = requests.post(LINE_PUSH_URL, headers=LINE_HEADERS, json=payload, timeout=10)

        if resp.status_code == 200:
            log.info(f"[ACE Today] Pushed {len(ids)} box IDs to ACE group. force={force}")
            # æ’ç¨‹ï¼ˆé forceï¼‰æˆåŠŸæ‰å¯« guardï¼›æ‰‹å‹•ï¼ˆforceï¼‰ä¸å¯«ï¼Œé¿å…å½±éŸ¿ç•¶æ—¥æ­£å¼æ¨æ’­
            if not force:
                r.set(guard_key, "1", ex=48 * 3600)
        else:
            log.error(f"[ACE Today] Push failed: {resp.status_code} {resp.text}")

    except Exception as e:
        log.error(f"[ACE Today] Error: {e}", exc_info=True)

# â”€â”€â”€ Heroku Scheduler hourly tick for ACE (stat-holiday style) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def ace_today_cron_tick():
    """
    è¢« Heroku Schedulerã€æ¯å°æ™‚ã€‘å‘¼å«ä¸€æ¬¡ã€‚
    åªæœ‰åœ¨ã€é€±å››æˆ–é€±æ—¥ã€ä¸”ã€16:00ï¼ˆAmerica/Vancouverï¼‰ã€æ™‚ï¼Œæ‰çœŸæ­£å‘¼å«
    push_ace_today_shipments(force=False)ã€‚å»é‡äº¤çµ¦å‡½å¼å…§å»ºçš„ Redis guardã€‚
    """
    tz = pytz.timezone(TIMEZONE)
    now = datetime.now(tz)

    # é€±å››=3ã€é€±æ—¥=6ï¼›åƒ…åœ¨ç•¶åœ° 16:00 æ™‚è§¸ç™¼
    if now.weekday() not in (3, 6) or now.hour != 16:
        log.info(f"[ACE Today TICK] skip at {now.strftime('%Y-%m-%d %H:%M:%S %Z')}")
        return

    log.info("[ACE Today TICK] due window hit; calling push_ace_today_shipments(force=False)")
    try:
        push_ace_today_shipments(force=False)
    except Exception as e:
        log.error(f"[ACE Today TICK] invoke failed: {e}", exc_info=True)

def _sq_collect_today_box_ids_by_tab(sheet_url: str) -> list[str]:
    """
    é–‹ SQ è©¦ç®—è¡¨ï¼Œä¾ä»Šå¤©æ—¥æœŸï¼ˆAmerica/Vancouverï¼‰ï¼Œæ‰¾åŒååˆ†é ï¼ˆYYMMDDï¼‰ï¼Œ
    æƒæè©²åˆ†é ï¼šè‹¥ã€Œæ¬„ C ä¸ç‚ºç©ºã€ï¼Œæ”¶é›†ã€Œæ¬„ Aã€ä½œç‚º Box IDã€‚å›å‚³å»é‡å¾Œæ¸…å–®ã€‚
    """
    gs = get_gspread_client()
    ss = gs.open_by_url(sheet_url)

    # ä»Šå¤© â†’ è½‰ tab åç¨±ï¼ˆYYMMDDï¼‰ï¼Œä¾‹å¦‚ 2025-10-10 â†’ 251010
    tz = pytz.timezone(TIMEZONE)
    today_local = datetime.now(tz).date()
    tab_name = today_local.strftime("%y%m%d")

    try:
        ws = ss.worksheet(tab_name)
    except Exception:
        # æ‰¾ä¸åˆ°ä»Šå¤©åˆ†é å°±å›ç©º
        return []

    rows = ws.get_all_values()  # 2D arrayï¼ˆå«è¡¨é ­ï¼‰
    box_ids = []
    # è‹¥ç¬¬ä¸€åˆ—æ˜¯è¡¨é ­å¯å¾ç¬¬äºŒåˆ—é–‹å§‹ï¼›ç„¡è¡¨é ­å¯å¾ç¬¬ä¸€åˆ—é–‹å§‹
    for row in rows[1:]:
        # æ¬„ä½ä¿è­·
        col_a = (row[0] if len(row) > 0 else "").strip()  # A: Box ID
        col_c = (row[2] if len(row) > 2 else "").strip()  # C: ä¸ç‚ºç©ºæ‰ç®—
        if col_a and col_c:
            box_ids.append(col_a)

    # å»é‡ï¼ˆä¿ç•™é †åºï¼‰
    seen = set()
    uniq = []
    for x in box_ids:
        if x not in seen:
            seen.add(x)
            uniq.append(x)
    return uniq

def push_sq_weekly_shipments(*, force: bool = False, reply_token: str | None = None):
    """
    æ¨æ’­ SQã€Œæœ¬é€±å‡ºè²¨ã€è¨Šæ¯åˆ° SQ ç¾¤çµ„ã€‚
    - ä¾†æºï¼šSQ_SHEET_URL æŒ‡å‘ä¹‹ Google Sheetï¼Œæ‰¾ä»Šå¤©æ—¥æœŸçš„ tabï¼ˆYYMMDDï¼‰
    - é‚è¼¯ï¼šè®€å– tab å…§ æ¬„Aï¼ˆBox IDï¼‰ï¼ŒåŒè¡Œæ¬„Cä¸ç‚ºç©ºè€…ï¼Œæ”¶é›†æ¬„A
    - æ™‚é–“ï¼šæ¯é€±å…­ 09:00 America/Vancouverï¼ˆæ’ç¨‹æœƒä»¥ force=False å‘¼å«ï¼‰
    - force=Trueï¼šä¸å¯« guardï¼Œå¯ç”¨ reply_token æä¾›å›è¦†ï¼›ç„¡è³‡æ–™ä¹Ÿæœƒå›ã€Œï¼ˆæ¸¬è©¦ï¼‰ã€å­—æ¨£
    - force=Falseï¼šå¯« guardï¼ˆ48h éæœŸï¼‰ï¼Œç„¡è³‡æ–™æ™‚ä¸åµç¾¤çµ„
    """
    tz = pytz.timezone(TIMEZONE)
    today_str = datetime.now(tz).strftime("%Y-%m-%d")
    guard_key = f"sq_weekly_shipments_pushed_{today_str}"

    # æ‰‹å‹•æ¸¬è©¦ï¼šå…ˆç§’å›ï¼ˆé«”é©—è¼ƒå³æ™‚ï¼‰
    if reply_token:
        try:
            requests.post(
                LINE_REPLY_URL,
                headers=LINE_HEADERS,
                json={"replyToken": reply_token, "messages": [{"type": "text", "text": "å·²æ¥æ”¶ï¼Œæ­£åœ¨æª¢æŸ¥ SQ ä»Šæ—¥å‡ºè²¨â€¦"}]},
                timeout=10,
            )
        except Exception as e:
            log.warning(f"[SQ Weekly] reply (pre-ack) failed: {e}")

    # æ’ç¨‹æ¨¡å¼é¿å…é‡è¤‡
    if not force and r.get(guard_key):
        log.info("[SQ Weekly] Already pushed for today; skipping.")
        return

    try:
        if not SQ_SHEET_URL:
            log.error("[SQ Weekly] SQ_SHEET_URL not set")
            return

        ids = _sq_collect_today_box_ids_by_tab(SQ_SHEET_URL)

        # è¨Šæ¯å…§å®¹
        if ids:
            base_text = "ä»Šæ—¥å‡ºè²¨ï¼š" + ", ".join(ids)
            text = base_text if not force else (base_text + "ï¼ˆæ¸¬è©¦ï¼‰")
        else:
            # ç„¡è³‡æ–™ï¼šforce æ™‚å›æ¸¬è©¦è¨Šæ¯ï¼›é force åƒ…å¯« guard
            if force:
                try:
                    if reply_token:
                        requests.post(
                            LINE_REPLY_URL,
                            headers=LINE_HEADERS,
                            json={"replyToken": reply_token, "messages": [{"type": "text", "text": "ä»Šæ—¥å‡ºè²¨ï¼šç›®å‰ç„¡è³‡æ–™ï¼ˆæ¸¬è©¦ï¼‰"}]},
                            timeout=10,
                        )
                    else:
                        requests.post(
                            LINE_PUSH_URL,
                            headers=LINE_HEADERS,
                            json={"to": SOQUICK_GROUP_ID, "messages": [{"type": "text", "text": "ä»Šæ—¥å‡ºè²¨ï¼šç›®å‰ç„¡è³‡æ–™ï¼ˆæ¸¬è©¦ï¼‰"}]},
                            timeout=10,
                        )
                except Exception as e:
                    log.error(f"[SQ Weekly] Manual test (no data) notify failed: {e}")
                return
            else:
                log.info("[SQ Weekly] No box IDs for today; nothing to push.")
                r.set(guard_key, "1", ex=48 * 3600)
                return

        # é€å‡ºæ¨æ’­ï¼ˆSQ ç¾¤ï¼‰
        payload = {"to": SOQUICK_GROUP_ID, "messages": [{"type": "text", "text": text}]}
        resp = requests.post(LINE_PUSH_URL, headers=LINE_HEADERS, json=payload, timeout=10)

        if resp.status_code == 200:
            log.info(f"[SQ Weekly] Pushed {len(ids)} box IDs to SQ group. force={force}")
            if not force:
                r.set(guard_key, "1", ex=48 * 3600)
        else:
            log.error(f"[SQ Weekly] Push failed: {resp.status_code} {resp.text}")

    except Exception as e:
        log.error(f"[SQ Weekly] Error: {e}", exc_info=True)

# â”€â”€â”€ Heroku Scheduler hourly tick for SQ (stat-holiday style) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def sq_weekly_cron_tick():
    """
    è¢« Heroku Schedulerã€æ¯å°æ™‚ã€‘å‘¼å«ä¸€æ¬¡ã€‚
    åƒ…åœ¨ã€Œé€±å…­ 09:00ï¼ˆAmerica/Vancouverï¼‰ã€æ™‚ï¼Œæ‰çœŸæ­£å‘¼å« push_sq_weekly_shipments(force=False)ã€‚
    å€ŸåŠ© push_sq_weekly_shipments å…§å»ºçš„ 48h guardï¼Œç¢ºä¿åªç™¼ä¸€æ¬¡ã€ä¸é‡è¤‡ã€‚
    """
    tz = pytz.timezone(TIMEZONE)
    now = datetime.now(tz)

    # åªåœ¨ã€é€±å…­ã€ä¸”ã€09:00ã€é€™ä¸€å°æ™‚å…§åšäº‹ï¼›å…¶ä»–æ™‚é–“ç›´æ¥ç•¥é
    if now.weekday() != 5 or now.hour != 9:
        log.info(f"[SQ Weekly TICK] skip at {now.isoformat()}")
        return

    # é€™è£¡ä¸ç›´æ¥æ“ä½œ guardï¼Œçµ±ä¸€äº¤çµ¦ push_sq_weekly_shipments() å…§éƒ¨è™•ç†
    log.info("[SQ Weekly TICK] due window hit; calling push_sq_weekly_shipments(force=False)")
    try:
        push_sq_weekly_shipments(force=False)
    except Exception as e:
        log.error(f"[SQ Weekly TICK] invoke failed: {e}", exc_info=True)


# â”€â”€ APScheduler è¨»å†Šï¼šæ¯é€±å…­ 09:00 America/Vancouver è§¸ç™¼ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
_sq_scheduler = None
def _ensure_scheduler_for_sq_weekly():
    """
    ä»¥èƒŒæ™¯æ’ç¨‹æ–¹å¼ï¼Œå›ºå®šæ¯é€±å…­ 09:00ï¼ˆAmerica/Vancouverï¼‰åŸ·è¡Œ
    push_sq_weekly_shipments(force=False)ã€‚
    """
    global _sq_scheduler
    if _sq_scheduler is not None:
        return _sq_scheduler

    tz = pytz.timezone(TIMEZONE)
    sched = BackgroundScheduler(timezone=tz)
    sched.add_job(
        push_sq_weekly_shipments,
        trigger="cron",
        day_of_week="sat",
        hour=9,
        minute=0,
        kwargs={"force": False},
        id="sq_weekly_shipments_sat_9am",
        replace_existing=True,
        misfire_grace_time=600,
        coalesce=True,
        max_instances=1,
    )
    sched.start()
    atexit.register(lambda: sched.shutdown(wait=False))
    log.info("[SQ Weekly] Scheduler started (Sat 09:00 America/Vancouver).")

    _sq_scheduler = sched
    return _sq_scheduler

def get_gspread_client():
    """Authorize gspread using env vars. Prefers GCP_SA_JSON_BASE64; falls back to GOOGLE_SVCKEY_JSON."""
    global _gs
    if _gs is not None:
        return _gs

    # Prefer the base64 var you added on Heroku
    b64 = os.getenv("GCP_SA_JSON_BASE64", "")
    json_inline = os.getenv("GOOGLE_SVCKEY_JSON", "")

    if b64:
        info = json.loads(base64.b64decode(b64))
    elif json_inline:
        # Back-compat: if you're still providing raw JSON text in GOOGLE_SVCKEY_JSON
        info = json.loads(json_inline)
    else:
        raise RuntimeError("Missing credentials: set GCP_SA_JSON_BASE64 (preferred) or GOOGLE_SVCKEY_JSON")

    creds = Credentials.from_service_account_info(info, scopes=SCOPES)

    # Only if you intentionally use Workspace domain-wide delegation:
    delegate = os.getenv("GSUITE_DELEGATE")
    if delegate:
        creds = creds.with_subject(delegate)

    _gs = gspread.authorize(creds)
    return _gs

# --- Debug: print SA client_email once on startup (safe) ---
try:
    import os, json, base64
    sa_json = None
    if os.getenv("GCP_SA_JSON_BASE64"):
        sa_json = base64.b64decode(os.getenv("GCP_SA_JSON_BASE64")).decode("utf-8", "ignore")
    elif os.getenv("GOOGLE_SVCKEY_JSON"):
        sa_json = os.getenv("GOOGLE_SVCKEY_JSON")
    if sa_json:
        client_email = json.loads(sa_json).get("client_email")
        if client_email:
            print(f"[GSHEET] service account email = {client_email}")
except Exception as _e:
    print("[GSHEET] could not print service account email:", _e)

# â”€â”€â”€ Structured Logging Setup â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s %(levelname)s %(message)s"
)
log = logging.getLogger(__name__)

# â”€â”€â”€ Customer Mapping â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Map each LINE group to the list of lowercase keywords you filter on
CUSTOMER_FILTERS = {
    os.getenv("LINE_GROUP_ID_YUMI"):   ["yumi", "shu-yen"],
    os.getenv("LINE_GROUP_ID_VICKY"):  ["vicky","chia-chi"]
}

# â”€â”€â”€ Status Translations â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
TRANSLATIONS = {
    "out for delivery today":         "ä»Šæ—¥æ´¾é€ä¸­",
    "out for delivery":               "æ´¾é€ä¸­",
    "processing at ups facility":     "UPSè™•ç†ä¸­",
    "arrived at facility":            "å·²åˆ°é”æ´¾é€ä¸­å¿ƒ",
    "departed from facility":         "å·²é›¢é–‹æ´¾é€ä¸­å¿ƒ",
    "pickup scan":                    "å–ä»¶æƒæ",
    "your package is currently at the ups access pointâ„¢ and is scheduled to be tendered to ups.": 
                                      "è²¨ä»¶ç›®å‰åœ¨ UPS å–è²¨é»ï¼Œç¨å¾Œå°‡äº¤äºˆ UPS",
    "drop-off":                       "å·²å¯„ä»¶",
    "order created at triple eagle":  "å·²åœ¨ç³»çµ±å»ºç«‹è¨‚å–®",
    "shipper created a label, ups has not received the package yet.": 
                                      "å·²å»ºç«‹é‹å–®ï¼ŒUPS å°šæœªæ”¶ä»¶",
    "delivered":                      "å·²é€é”",
}

# æ¨¡çµ„è¼‰å…¥æ™‚å°±ç¢ºä¿ SQ æ’ç¨‹å•Ÿå‹•ï¼ˆèˆ‡ ACE çš„ _ensure_scheduler_for_ace_today ä¸¦å­˜ï¼‰
try:
    _ensure_scheduler_for_sq_weekly()
except Exception as _e:
    log.error(f"[SQ Weekly] Scheduler init failed: {_e}")

# â”€â”€ APScheduler è¨»å†Šï¼šæ¯é€±å››ï¼†é€±æ—¥ 16:00 America/Vancouver è§¸ç™¼ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
_scheduler = None
def _ensure_scheduler_for_ace_today():
    """
    ä»¥èƒŒæ™¯æ’ç¨‹æ–¹å¼ï¼Œå›ºå®šåœ¨æ¯é€±å››èˆ‡é€±æ—¥çš„ 16:00ï¼ˆAmerica/Vancouverï¼‰åŸ·è¡Œ
    push_ace_today_shipments(force=False)ã€‚
    - ä½¿ç”¨ coalesce / max_instances ä¾†é¿å…é‡å•Ÿé€ æˆçš„å †ç–Šè§¸ç™¼
    - ä½¿ç”¨ misfire_grace_time å…è¨±çŸ­æš«å–šé†’å»¶é²
    """
    global _scheduler
    if _scheduler is not None:
        return _scheduler

    tz = pytz.timezone(TIMEZONE)
    sched = BackgroundScheduler(timezone=tz)
    sched.add_job(
        push_ace_today_shipments,
        trigger="cron",
        day_of_week="thu,sun",
        hour=16,
        minute=0,
        kwargs={"force": False},  # æ’ç¨‹å‘¼å«ï¼Œä¸€å¾‹é force
        id="ace_today_shipments_thu_sun_4pm",
        replace_existing=True,
        misfire_grace_time=600,
        coalesce=True,
        max_instances=1,
    )
    sched.start()
    atexit.register(lambda: sched.shutdown(wait=False))
    log.info("[ACE Today] Scheduler started (Thu/Sun 16:00 America/Vancouver).")

    _scheduler = sched
    return _scheduler

# æ¨¡çµ„åŒ¯å…¥æ™‚å°±ç¢ºä¿æ’ç¨‹å•Ÿå‹•ï¼ˆå¤šæ¬¡åŒ¯å…¥ä¹Ÿå®‰å…¨ï¼‰
try:
    _ensure_scheduler_for_ace_today()
except Exception as _e:
    log.error(f"[ACE Today] Scheduler init failed: {_e}")

# â”€â”€â”€ ADDED: Configure OpenAI API key â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
openai.api_key = os.getenv("OPENAI_API_KEY")
OPENAI_MODEL = os.getenv("OPENAI_MODEL", "gpt-4o-mini")

OCR_SHIPPING_PROMPT = """
Task: Extract the following information from this shipping ticket
- Information of Sender on the `top-right corner`:
  - name
  - phone
  - client ID (the text on the third line between phone and address)
  - address
- Information of Receiver in the `SHIP TO` section
  - postal code(format `SNS NSN`, N stand for number and S stand for english character)
- Reference Number at the bottom after `Reference No.1:`
  - reference number
Response Format: {"sender": {"name": "", "phone": "", "client_id": "", "address": ""}, "receiver": {"postal_code": ""}, "reference number": ""}
* Do not include any extra text, explanation, or JSON outside of this format.
"""

TRACKING_PROMPT = """
Task: From this image of a shipping ticket page, extract ONLY the UPS tracking number.
The tracking number always starts with "1Z" and is alphanumeric.

Response Format (pure JSON):
{"tracking_number": ""}
* Do not include extra text or other fields.
"""

# keep an in-memory buffer of successfully updated tracking IDs per group
_pending = defaultdict(list)
_scheduled = set()

def strip_mention(line):
    # Remove an @mention at the very start of the line (e.g. "@Gorsky ")
    return re.sub(r"^@\S+\s*", "", line)

def _schedule_summary(group_id):
    """Called once per 30m window to send the summary and clear the buffer."""
    ids = _pending.pop(group_id, [])
    _scheduled.discard(group_id)
    if not ids:
        return
    # dedupe and format
    uniq = sorted(set(ids))
    text = "âœ… Updated packages:\n" + "\n".join(f"- {tid}" for tid in uniq)
    payload = {
        "to": group_id,
        "messages": [{"type": "text", "text": text}]
    }
    requests.post(LINE_PUSH_URL, headers=LINE_HEADERS, json=payload)


# â”€â”€â”€ Signature Generator â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def generate_sign(params: dict, secret: str) -> str:
    # Build encodeURIComponent-style querystring
    parts = []
    for k in sorted(params.keys()):
        v = params[k]
        parts.append(f"{k}={quote(str(v), safe='~')}")
    qs = "&".join(parts)

    # HMAC-SHA256 and Base64-encode
    sig_bytes = hmac.new(secret.encode(), qs.encode(), hashlib.sha256).digest()
    return base64.b64encode(sig_bytes).decode('utf-8')

# â”€â”€â”€ TripleEagle API Caller â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def call_api(action: str, payload: dict = None) -> dict:
    ts = str(int(datetime.now().timestamp()))
    params = {"id": APP_ID, "timestamp": ts, "format": "json", "action": action}
    params["sign"] = generate_sign(params, APP_SECRET)
    url = "https://eship.tripleeaglelogistics.com/api?" + "&".join(
        f"{k}={quote(str(params[k]), safe='~')}" for k in params
    )
    headers = {"Content-Type": "application/json"}
    if payload:
        r = requests.post(url, json=payload, headers=headers)
    else:
        r = requests.get(url, headers=headers)
    r.raise_for_status()
    return r.json()

# Helper for sending single final LINE message for uploading PDF
def _line_push(to: str, text: str):
    try:
        requests.post(
            "https://api.line.me/v2/bot/message/push",
            headers=LINE_HEADERS,
            json={"to": to, "messages": [{"type": "text", "text": text}]},
            timeout=10,
        )
    except Exception as e:
        log.error(f"[LINE PUSH] failed: {e}")

def process_pdf_ocr_job(*, file_id: str, group_id: str, original_filename: str = "uploaded.pdf"):
    """
    Runs off-request. Downloads the PDF from LINE, converts to images,
    runs OCR with OpenAI, then pushes results to the originating group.
    """
    try:
        # 1) Download PDF
        resp = requests.get(
            f"https://api-data.line.me/v2/bot/message/{file_id}/content",
            headers={"Authorization": f"Bearer {LINE_TOKEN}"},
            timeout=30,
        )
        resp.raise_for_status()
        pdf_bytes = resp.content

        # 2) Render to images (prefer pdf2image, fallback to PyMuPDF)
        try:
            images = convert_from_bytes(pdf_bytes, dpi=200)
            if not images:
                raise ValueError("no images from pdf2image")
            log.info(f"[PDF OCR/JOB] pdf2image rendered {len(images)} pages")
        except Exception:
            log.warning("[PDF OCR/JOB] convert_from_bytes failed, fallback to PyMuPDF")
            images = []
            doc = fitz.open(stream=pdf_bytes, filetype="pdf")
            for p in range(doc.page_count):
                page = doc.load_page(p)
                pix = page.get_pixmap(matrix=fitz.Matrix(2, 2))
                mode = "RGBA" if pix.alpha else "RGB"
                img = Image.frombytes(mode, (pix.width, pix.height), pix.samples)
                images.append(img)
            log.info(f"[PDF OCR/JOB] PyMuPDF rendered {len(images)} pages")

        if not images:
            _line_push(group_id, f"âš ï¸ OCR å¤±æ•—ï¼š{original_filename} ç„¡æ³•è½‰åœ–åƒ")
            return

        # 3) OCR the pages using your existing helper
        full_data = {}
        tracking_numbers = []

        for idx, img in enumerate(images, start=1):
            if idx == 1:
                full_data_raw = extract_text_from_images(img, prompt=OCR_SHIPPING_PROMPT)
                # tolerate {"_raw": "..."} from extractor
                if isinstance(full_data_raw, dict) and "_raw" in full_data_raw:
                    parsed = parse_json_forgiving(full_data_raw["_raw"])
                    if "tracking number" in full_data_raw:
                        parsed["tracking_number"] = full_data_raw["tracking number"]
                    full_data = parsed
                else:
                    full_data = full_data_raw if isinstance(full_data_raw, dict) else parse_json_forgiving(full_data_raw)
            else:
                r = extract_text_from_images(img, prompt=TRACKING_PROMPT)
                parsed = r if isinstance(r, dict) else parse_json_forgiving(r)
                tn = parsed.get("tracking_number") or parsed.get("tracking number")
                if tn:
                    tracking_numbers.append(normalize_ups(tn))

        # 4) Compose push text
        tn1 = full_data.get("tracking_number") or full_data.get("tracking number")
        if tn1:
            tracking_numbers.insert(0, normalize_ups(tn1))
        tracking_numbers = [t for t in tracking_numbers if t]

        sender = full_data.get("sender") or {}
        receiver = full_data.get("receiver") or {}
        ref_no = full_data.get("reference number") or full_data.get("reference_number") or ""

        lines = [f"ğŸ“„ OCR å®Œæˆï¼š{original_filename}"]
        if sender:
            lines += [
                "ğŸ‘¤ Sender",
                f"â€¢ name: {sender.get('name','')}",
                f"â€¢ phone: {sender.get('phone','')}",
                f"â€¢ client_id: {sender.get('client_id','')}",
                f"â€¢ address: {sender.get('address','')}",
            ]
        if receiver:
            lines += ["ğŸ“¬ Receiver", f"â€¢ postal_code: {receiver.get('postal_code','')}"]
        if ref_no:
            lines += [f"ğŸ”– Reference No.1: {ref_no}"]
        if tracking_numbers:
            lines += ["ğŸ”¢ Tracking", *[f"â€¢ {t}" for t in tracking_numbers]]

        msg = "\n".join(lines) if len(lines) > 1 else "âœ… OCR å®Œæˆ"

        # 5) Push result
        _line_push(group_id, msg)
        log.info(f"[PDF OCR/JOB] pushed result for {original_filename} to group {group_id}")

    except Exception as e:
        log.exception(f"[PDF OCR/JOB] error: {e}")
        _line_push(group_id, f"âš ï¸ OCR å¤±æ•—ï¼š{original_filename}ï¼Œè«‹ç¨å¾Œå†è©¦")


# â”€â”€â”€ Business Logic â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def get_statuses_for(keywords: list[str]) -> list[str]:
    # 1) list all active orders
    resp = call_api("shipment/list")
    lst  = resp.get("response", {}).get("list") or resp.get("response") or []
    order_ids = [o["id"] for o in lst if "id" in o]
    # 2) filter by these keywords
    cust_ids = []
    for oid in order_ids:
        det = call_api("shipment/detail", {"id": oid}).get("response", {})
        if isinstance(det, list): det = det[0]
        init = det.get("initiation", {})
        loc  = next(iter(init), None)
        name = init.get(loc,{}).get("name","").lower() if loc else ""
        if any(kw in name for kw in keywords):
            cust_ids.append(oid)
    if not cust_ids:
        return ["ğŸ“¦ æ²’æœ‰æ­¤å®¢æˆ¶çš„æœ‰æ•ˆè¨‚å–®"]
    # 3) fetch tracking updates
    td = call_api("shipment/tracking", {
        "keyword": ",".join(cust_ids),
        "rsync":   0,
        "timezone": TIMEZONE
    })
    # 4) format reply using each eventâ€™s own timestamp
    lines: list[str] = []
    for item in td.get("response", []):
        oid = item.get("id"); num = item.get("number","")
        events = item.get("list") or []
        if not events:
            lines.append(f"ğŸ“¦ {oid} ({num}) â€“ å°šç„¡è¿½è¹¤ç´€éŒ„")
            continue
        # pick the most recent event
        ev = max(events, key=lambda e: int(e["timestamp"]))
        loc_raw    = ev.get("location","")
        loc        = f"[{loc_raw.replace(',',', ')}] " if loc_raw else ""
        ctx_lc     = ev.get("context","").strip().lower()
        translated = TRANSLATIONS.get(ctx_lc, ev.get("context","").replace("Triple Eagle","system"))

        # derive the *real* event time from its epoch timestamp
        # 1) parse the numeric timestamp
        event_ts = int(ev["timestamp"])
        # 2) convert to a timezoneâ€aware datetime
        #    (make sure you have `import pytz` and `from datetime import datetime` at the top)
        tzobj = pytz.timezone(TIMEZONE)
        dt = datetime.fromtimestamp(event_ts, tz=tzobj)
        # 3) format it exactly like "Wed, 11 Jun 2025 15:05:46 -0700"
        tme = dt.strftime('%a, %d %b %Y %H:%M:%S %z')

        lines.append(f"ğŸ“¦ {oid} ({num}) â†’ {loc}{translated}  @ {tme}")
    return lines

MONDAY_API_URL    = "https://api.monday.com/v2"
MONDAY_API_TOKEN  = os.getenv("MONDAY_API_TOKEN")
VICKY_SUBITEM_BOARD_ID = 4815120249    # è«‹å¡«ä½  Vicky å­ä»»å‹™æ‰€åœ¨çš„ Board ID
VICKY_STATUS_COLUMN_ID = "status__1"   # è«‹å¡«æº«å“¥è¯æ”¶æ¬¾é‚£å€‹æ¬„ä½çš„ column_id

# â”€â”€â”€ Vicky-reminder helpers â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€(under construction)    
def vicky_has_active_orders() -> list[str]:
    """
    Return a list of Vickyâ€™s active UPS tracking numbers (the 1Zâ€¦ codes).
    """
    # include parent_item.name so we can filter only Vickyâ€™s
    query = '''
    query ($boardId: ID!, $columnId: String!, $value: String!) {
      items_page_by_column_values(
        board_id: $boardId,
        limit: 100,
        columns: [{ column_id: $columnId, column_values: [$value] }]
      ) {
        items {
          name
          parent_item { name }
        }
      }
    }
    '''
    # æŸ¥è©¢å¤šç¨®éœ€æé†’çš„ç‹€æ…‹
    statuses = ["æ”¶åŒ…è£¹", "æ¸¬é‡", "é‡æ–°åŒ…è£", "æä¾›è³‡æ–™", "æº«å“¥è¯æ”¶æ¬¾"]
    to_remind = []
    
    for status in statuses:
        log.info(f"[vicky_has_active_orders] querying status {status!r}")
        resp = requests.post(
            MONDAY_API_URL,
            headers={ "Authorization": f"Bearer {MONDAY_API_TOKEN}", "Content-Type": "application/json" },
            json={ "query": query, "variables": {
                "boardId": VICKY_SUBITEM_BOARD_ID,
                "columnId": VICKY_STATUS_COLUMN_ID,
                "value": status
            }}
        )
        items = resp.json()\
                   .get("data", {})\
                   .get("items_page_by_column_values", {})\
                   .get("items", [])

        # keep only Vickyâ€™s
        filtered = [
            itm["name"].strip()
            for itm in items
            if itm.get("parent_item", {}).get("name", "").find("Vicky") != -1
        ]
        log.info(f"[vicky_has_active_orders] {len(filtered)} of {len(items)} are Vickyâ€™s for {status!r}")
        to_remind.extend(filtered)
    
    # å»é‡æ’åº
    to_remind = sorted(set(to_remind))
    
    if not to_remind:
      return []

    # 3) We already have the subitem names (tracking IDs) in to_remind:
    return to_remind

# â”€â”€â”€ Wednesday/Friday reminder callback â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def remind_vicky(day_name: str):
    log.info(f"[remind_vicky] Called for {day_name}")
    tz = pytz.timezone(TIMEZONE)
    today_str = datetime.now(tz).date().isoformat()
    guard_key = f"vicky_reminder_{day_name}_{today_str}"
    log.info(f"[remind_vicky] guard_key={guard_key!r}, existing={r.get(guard_key)!r}")
    if r.get(guard_key):
        log.info("[remind_vicky] Skipping because guard is set")
        return  
         
    # 1) Grab Monday subitems in the statuses you care about
    to_remind_ids = vicky_has_active_orders()  # returns list of TE IDs from Monday
    log.info(f"[remind_vicky] vicky_has_active_orders â†’ {to_remind_ids!r}")
    if not to_remind_ids:
        log.info("[remind_vicky] No subitems in statuses to remind, exiting")
        return

    # 2) Use the subitem names directly as the list to remind
    to_remind = to_remind_ids

    if not to_remind:
        log.info("[remind_vicky] No tracking numbers found, exiting")
        return

    # â”€â”€ 3) Assemble and send reminder (no sheet link) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    placeholder = "{user1}"
    header = (
        f"{placeholder} æ‚¨å¥½ï¼Œæº«å“¥è¯å€‰åº«é è¨ˆ{day_name}å‡ºè²¨ï¼Œ"
        "è«‹éº»ç…©å¡«å¯«ä»¥ä¸‹åŒ…è£¹çš„å†…å®¹ç‰©æ¸…å–®ã€‚è¬è¬ï¼"
    )
    body = "\n".join(to_remind)
    payload = {
        "to": VICKY_GROUP_ID,
        "messages": [{
            "type":        "textV2",
            "text":        "\n\n".join([header, body]),
            "substitution": {
                "user1": {
                    "type": "mention",
                    "mentionee": {
                        "type":   "user",
                        "userId": VICKY_USER_ID
                    }
                }
            }
        }]
    }
    try:
        resp = requests.post(LINE_PUSH_URL, headers=LINE_HEADERS, json=payload)
        if resp.status_code == 200:
            # mark as sent for today
            r.set(guard_key, "1", ex=24*3600)
            log.info(f"Sent Vicky reminder for {day_name}: {len(to_remind)} packages")
        else:
            log.error(f"Failed to send Vicky reminder: {resp.status_code} {resp.text}")
    except Exception as e:
        log.error(f"Error sending Vicky reminder: {e}")

# def vicky_sheet_recently_edited():
    ##1) build a credentials object from your SERVICE_ACCOUNT JSON
    # creds = Credentials.from_service_account_info(
        # json.loads(os.environ["GOOGLE_SVCKEY_JSON"]),
        # scopes=SCOPES
    # )

    ##2) fetch the spreadsheetâ€™s Drive metadata
    # drive = build("drive", "v3", credentials=creds)
    # sheet_url = os.environ["VICKY_SHEET_URL"]
    # file_id = sheet_url.split("/")[5]            # extract the ID from the URL
    # meta = drive.files().get(
        # fileId=file_id,
        # fields="modifiedTime"
    # ).execute()

    ##3) parse the ISO timestamp into a datetime
    # last_edit = datetime.fromisoformat(meta["modifiedTime"].replace("Z","+00:00"))

    ##4) compare against now (UTC)
    # age = datetime.now(timezone.utc) - last_edit
    # return age.days < 3
  
def handle_ace_ezway_check_and_push_to_yves(event):
    """
    For any ACE message that contains â€œéº»ç…©è«‹â€ + â€œæ”¶åˆ°EZ wayé€šçŸ¥å¾Œâ€ + (é€±å››å‡ºè²¨ or é€±æ—¥å‡ºè²¨),
    we will look up the *sheet* for the row whose date is closest to today, but ONLY
    for those â€œdeclaring personsâ€ that actually appeared in the ACE text.  For each
    matching row, we pull the â€œsenderâ€ (column C) and push it privately if it's not in
    VICKY_NAMES or YUMI_NAMES or EXCLUDED_SENDERS.
    """
    text = event["message"]["text"]

    # Only trigger on the exact keywords
    if not (
        "éº»ç…©è«‹" in text
        and "æ”¶åˆ°EZ wayé€šçŸ¥å¾Œ" in text
        and ("é€±å››å‡ºè²¨" in text or "é€±æ—¥å‡ºè²¨" in text)
    ):
        return

    # â”€â”€ 1) Extract declarerâ€names from the ACE text â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    lines = text.splitlines()

    # find the line index that contains â€œéº»ç…©è«‹â€
    try:
        idx_m = next(i for i, l in enumerate(lines) if "éº»ç…©è«‹" in l)
    except StopIteration:
        # If we can't find it, default to the top
        idx_m = 0

    # find the line index that starts with â€œæ”¶åˆ°EZ wayé€šçŸ¥å¾Œâ€
    try:
        idx_r = next(i for i, l in enumerate(lines) if l.startswith("æ”¶åˆ°EZ wayé€šçŸ¥å¾Œ"))
    except StopIteration:
        idx_r = len(lines)

    # declarer lines are everything strictly between â€œéº»ç…©è«‹â€ and â€œæ”¶åˆ°EZ wayé€šçŸ¥å¾Œâ€
    raw_declarer_lines = lines[idx_m+1 : idx_r]
    declarer_names = set()

    for line in raw_declarer_lines:
        # Remove any ACEâ€style code prefix (e.g. â€œACE250605YL04 â€)
        cleaned = CODE_TRIGGER_RE.sub("", line).strip().strip('"')
        if not cleaned:
            continue

        # Take the first â€œtokenâ€ as the actual name (before any phone or other columns)
        name_token = cleaned.split()[0]
        if name_token:
            declarer_names.add(name_token)

    if not declarer_names:
        # No valid declarers found in the message â†’ nothing to do
        return

    # â”€â”€ 2) Open the ACE sheet and find the â€œclosestâ€dateâ€ row â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    ACE_SHEET_URL = os.getenv("ACE_SHEET_URL")
    gs = get_gspread_client()
    sheet = gs.open_by_url(ACE_SHEET_URL).sheet1
    data = sheet.get_all_values()  # raw rows as lists of strings

    today = datetime.now(timezone.utc).date()
    closest_date = None
    closest_diff = timedelta(days=9999)

    # Assume column A is date; skip header row at index 0, so start at row 2 in the sheet
    for row_idx, row in enumerate(data[1:], start=2):
        date_str = row[0].strip()
        if not date_str:
            continue
        try:
            row_date = parse_date(date_str).date()
        except Exception:
            continue

        diff = abs(row_date - today)
        if diff < closest_diff:
            closest_diff = diff
            closest_date = row_date

    if closest_date is None:
        # No parseable dates in sheet â†’ bail out
        return

    # â”€â”€ 3) Scan only the rows on that closest_date, and only if column B (declarer)
    #         is in our declarer_names set.  Then we grab column C (sender) for private push.
    results = set()

    for row_idx, row in enumerate(data[1:], start=2):
        date_str = row[0].strip()
        if not date_str:
            continue
        try:
            row_date = parse_date(date_str).date()
        except Exception:
            continue

        if row_date != closest_date:
            continue

        # Column B is at index 1 in 'row'
        declarer = row[1].strip() if len(row) > 1 else ""
        if not declarer or declarer not in declarer_names:
            continue

        # Column C is at index 2 in 'row' â†’ this is the â€œsenderâ€ we want to notify
        sender = row[2].strip() if len(row) > 2 else ""
        if not sender:
            continue

        # Skip anyone already in VICKY_NAMES, YUMI_NAMES, or EXCLUDED_SENDERS
        if sender in VICKY_NAMES or sender in YUMI_NAMES or sender in EXCLUDED_SENDERS:
            continue

        results.add(sender)

    # â”€â”€ 4) Push to Yves privately if any senders remain â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    if results:
        header_payload = {
            "to": YVES_USER_ID,
            "messages": [{"type": "text", "text": "Aceæ•£å®¢EZWayéœ€æé†’ä»¥ä¸‹å¯„ä»¶äººï¼š"}]
        }
        requests.post(LINE_PUSH_URL, headers=LINE_HEADERS, json=header_payload)

        for sender in sorted(results):
            payload = {
                "to": YVES_USER_ID,
                "messages": [{"type": "text", "text": sender}]
            }
            requests.post(LINE_PUSH_URL, headers=LINE_HEADERS, json=payload)

        print(f"DEBUG: Pushed {len(results)} sender(s) to Yves: {sorted(results)}")
    else:
        print("DEBUG: No matching senders found for any declarer in the ACE message.")

# â”€â”€â”€ Soquick & Ace shipment-block handler â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def handle_soquick_and_ace_shipments(event):
    """
    Parse Soquick & Ace text containing "ä¸Šå‘¨å…­å‡ºè²¨åŒ…è£¹çš„æ´¾ä»¶å–®è™Ÿ", "å‡ºè²¨å–®è™Ÿ", "å®…é…å–®è™Ÿ"
    split out lines of tracking+code+recipient, then push
    only the matching Vicky/Yumi lines + footer.
    """
    raw = event["message"]["text"]
    if "ä¸Šå‘¨å…­å‡ºè²¨åŒ…è£¹çš„æ´¾ä»¶å–®è™Ÿ" not in raw and not ("å‡ºè²¨å–®è™Ÿ" in raw and "å®…é…å–®è™Ÿ" in raw):
        return

    vicky, yumi = [], []

    # â€” Soquick flow â€”
    if "ä¸Šå‘¨å…­å‡ºè²¨åŒ…è£¹çš„æ´¾ä»¶å–®è™Ÿ" in raw:
        # Split into non-empty lines
        lines = [l.strip() for l in raw.splitlines() if l.strip()]
        # Locate footer (starts with â€œæ‚¨å¥½â€)
        footer_idx = next((i for i,l in enumerate(lines) if l.startswith("æ‚¨å¥½")), len(lines))
        header = lines[:footer_idx]
        footer = "\n".join(lines[footer_idx:])

        for line in header:
            parts = line.split()
            if len(parts) < 3:
                continue
            recipient = parts[-1]
            if recipient in VICKY_NAMES:
                vicky.append(line)
            elif recipient in YUMI_NAMES:
                yumi.append(line)

    # â€” Ace flow â€”
    else:
        # split into one block per â€œå‡ºè²¨å–®è™Ÿ:â€ line
        blocks = [b.strip().strip('"') for b in re.split(r'(?=å‡ºè²¨å–®è™Ÿ:)', raw) if b.strip()]
        
        for blk in blocks:
            # strip whitespace and any wrapping quotes
            block = blk.strip().strip('"')
            if not block:
                continue
            # must contain both å‡ºè²¨å–®è™Ÿ and å®…é…å–®è™Ÿ
            if "å‡ºè²¨å–®è™Ÿ" not in block or "å®…é…å–®è™Ÿ" not in block:
                continue
            lines = block.splitlines()
            if len(lines) < 3:
                continue
            recipient = lines[2].split()[0]
            if recipient in VICKY_NAMES:
                vicky.append(block)
            elif recipient in YUMI_NAMES:
                yumi.append(block)

    def push(group, msgs):
        if not msgs:
            return
        
        # choose formatting per flow
        if "ä¸Šå‘¨å…­å‡ºè²¨åŒ…è£¹çš„æ´¾ä»¶å–®è™Ÿ" in raw:
            text = "\n".join(msgs) + "\n\n" + footer
        else:
            text = "\n\n".join(msgs)
        payload = {"to": group, "messages":[{"type":"text","text": text}]}
        resp = requests.post(LINE_PUSH_URL, headers=LINE_HEADERS, json=payload)
        log.info(f"Sent {len(msgs)} Soquick blocks to {group}: {resp.status_code}")

    push(VICKY_GROUP_ID, vicky)
    push(YUMI_GROUP_ID,  yumi)



def handle_soquick_full_notification(event):



    log.info(f"[SOQ FULL] invoked on text={event['message']['text']!r}")
    text = event["message"]["text"]
    """
    1) Parse the incoming text for â€œæ‚¨å¥½ï¼Œè«‹â€¦â€ + â€œæŒ‰ç”³å ±ç›¸ç¬¦â€
    2) Split off the footer and extract all recipient names
    3) Push Vicky/Yumi group messages with their names + footer
    4) Look up those same names in col M of your Soquick sheet
       to find the corresponding senders in col C, and privately
       notify Yves of any senders not already in Vicky/Yumi/Excluded.
    """
    text = event["message"]["text"]
    if not ("æ‚¨å¥½ï¼Œè«‹" in text and "æŒ‰" in text and "ç”³å ±ç›¸ç¬¦" in text):
        return

    # 1) extract lines & footer
    # split into non-empty lines and strip any leading @mention
    lines = [
        strip_mention(l.strip())
        for l in text.splitlines()
        if l.strip()
    ]
    try:
        footer_idx = next(i for i,l in enumerate(lines) if "æ‚¨å¥½ï¼Œè«‹" in l)
    except StopIteration:
        footer_idx = len(lines)
    recipients = lines[:footer_idx]
    footer     = "\n".join(lines[footer_idx:])

    # 2) split into Vicky / Yumi / â€œothersâ€ batches
    vicky_batch = [r for r in recipients if r in VICKY_NAMES]
    yumi_batch  = [r for r in recipients if r in YUMI_NAMES]
    other_recipients = [
        r for r in recipients
        if r not in VICKY_NAMES
           and r not in YUMI_NAMES
           and r not in EXCLUDED_SENDERS
    ]

    # ===== æ’å…¥é€™è£¡ï¼šåˆ—å° other_recipients =====
    log.info(f"[SOQ FULL][DEBUG] other_recipients = {other_recipients!r}")

    # dedupe
    def dedupe(seq):
        seen = set(); out=[]
        for x in seq:
            if x not in seen:
                seen.add(x); out.append(x)
        return out
    vicky_batch = dedupe(vicky_batch)
    yumi_batch  = dedupe(yumi_batch)
    other_recipients = dedupe(other_recipients)

    # 3) push the group notifications
    def push_group(group, batch):
        if not batch: return
        standard_footer = "æ‚¨å¥½ï¼Œè«‹æé†’ä»¥ä¸Šèªè­‰äººæŒ‰ç”³å ±ç›¸ç¬¦"
        msg = "\n".join(batch) + "\n\n" + standard_footer
        requests.post(
            LINE_PUSH_URL,
            headers=LINE_HEADERS,
            json={"to": group, "messages":[{"type":"text","text":msg}]}
        )

    # é€™è¡Œå–æ¶ˆè¨»è§£å°±ä¸æœƒæ¨çµ¦ Vicky
    push_group(VICKY_GROUP_ID, vicky_batch)
    push_group(YUMI_GROUP_ID,  yumi_batch)

    # â”€â”€ Private â€œotherâ€ pushes â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    other_recipients = dedupe([
        r for r in recipients
        if r not in VICKY_NAMES
           and r not in YUMI_NAMES
           and r not in EXCLUDED_SENDERS
    ])
    log.info(f"[SOQ FULL][DEBUG] other_recipients = {other_recipients!r}")

    if other_recipients:
        # ä¾ç…§è¨Šæ¯æ—¥æœŸå‹•æ…‹é¸åˆ†é ï¼šå‰3å¤©åˆ°å¾Œ2å¤©
        import datetime
        ts = event["timestamp"]                              # ms
        dt = datetime.datetime.fromtimestamp(ts/1000,         # +08:00
            tz=datetime.timezone(datetime.timedelta(hours=8)))
        # å€™é¸æ—¥æœŸå­—ä¸²ï¼še.g. ['250611','250612','250613','250614','250615','250616']
        base = dt.date()
        candidates = [(base + datetime.timedelta(days=d)).strftime("%y%m%d")
                      for d in range(-3, 3)]

        #guard to ensure the sheet exist and don't crash
        SQ_SHEET_URL = os.getenv("SQ_SHEET_URL")
        if not SQ_SHEET_URL:
            log.error("[SOQ FULL] SQ_SHEET_URL not set")
            return        
        
        gs = get_gspread_client()
        ss = gs.open_by_url(SQ_SHEET_URL)
        found = [ws.title for ws in ss.worksheets() if ws.title in candidates]
        if len(found) == 1:
            sheet = ss.worksheet(found[0])
            log.info(f"[SOQ FULL][DEBUG] ä½¿ç”¨åˆ†é  {found[0]}")
        else:
            log.error(f"[SOQ FULL] åˆ†é æ•¸é‡ä¸å”¯ä¸€ï¼Œexpected=1 got={len(found)}; candidates={candidates}, found={found}")
            return
        rows = sheet.get_all_values()[1:]  # skip header
        senders = set()

        for idx, row in enumerate(rows, start=2):
            # å°æ¯ä¸€åˆ— E æ¬„
            name_in_sheet = row[4].strip() if len(row) > 4 else ""
            log.info(f"[SOQ FULL][DEBUG] row {idx} colE = {name_in_sheet!r}")

            if name_in_sheet in other_recipients:
                sender = row[2].strip() if len(row) > 2 else ""
                log.info(f"[SOQ FULL][DEBUG] matched recipient {name_in_sheet!r} â†’ sender {sender!r}")
                if sender and sender not in (VICKY_NAMES | YUMI_NAMES | EXCLUDED_SENDERS):
                    senders.add(sender)

        if senders:
            # header notification
            requests.post(
                LINE_PUSH_URL, headers=LINE_HEADERS,
                json={
                  "to": YVES_USER_ID,
                  "messages":[{"type":"text","text":"Soquickæ•£å®¢EZWayéœ€æé†’ä»¥ä¸‹å¯„ä»¶äººï¼š"}]
                }
            )
            for s in sorted(senders):
                requests.post(
                    LINE_PUSH_URL, headers=LINE_HEADERS,
                    json={"to": YVES_USER_ID, "messages":[{"type":"text","text":s}]}
                )
            log.info(f"[SOQ FULL] Privately pushed {len(senders)} senders to Yves")

 
# â”€â”€â”€ æ–°å¢ï¼šè™•ç†ã€Œç”³å ±ç›¸ç¬¦ã€æé†’ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def handle_missing_confirm(event):
    text = event["message"]["text"]
    
    # å¦‚æœé€™æ˜¯åŸå§‹ EZ-Way é€šçŸ¥ï¼Œå°±è·³é
    if "æ”¶åˆ°EZ wayé€šçŸ¥å¾Œ" in text:
        return
    
    # å¦‚æœè¨Šæ¯è£¡æ²’æœ‰ã€Œç”³å ±ç›¸ç¬¦ã€ï¼Œå°±è·³é
    if "ç”³å ±ç›¸ç¬¦" not in text:
        return
        
    # é€è¡Œæ‰¾ ACE/250N å–®è™Ÿ
    for l in text.splitlines():
        if CODE_TRIGGER_RE.search(l):
            parts = re.split(r"\s+", l.strip())
            # ç¢ºä¿è‡³å°‘æœ‰ä¸‰æ®µï¼šå–®è™Ÿã€å§“åã€é›»è©±
            if len(parts) < 2:
                continue
            name = parts[1]
            if name in VICKY_NAMES:
                target = VICKY_GROUP_ID
            elif name in YUMI_NAMES:
                target = YUMI_GROUP_ID
            else:
                # ä¸æ˜¯ Vicky ä¹Ÿä¸æ˜¯ Yumi çš„äººï¼Œç›´æ¥è·³é
                continue
                
            # æ¨æ’­å§“åï¼ˆä½ å¯ä»¥æ”¹æˆæ›´å®Œæ•´çš„è¨Šæ¯ï¼‰
            requests.post(
                LINE_PUSH_URL,
                headers=LINE_HEADERS,
                json={"to": target, "messages":[{"type":"text","text": f"{name} å°šæœªæŒ‰ç”³å ±ç›¸ç¬¦"}]}
            )

# â”€â”€â”€ Ace schedule handler â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def handle_ace_schedule(event):
    """
    Extracts the Ace message, filters lines for Yumi/Vicky,
    and pushes a cleaned summary into their groups with the names
    inserted between éº»ç…©è«‹ and æ”¶åˆ°EZ wayé€šçŸ¥å¾Œâ€¦
    """
    text     = event["message"]["text"]
    # split into lines
    lines = text.splitlines()

    # find the index of the â€œéº»ç…©è«‹â€ line
    try:
        idx_m = next(i for i,l in enumerate(lines) if "éº»ç…©è«‹" in l)
    except StopIteration:
        idx_m = 1  # fallback just after the first line

    # find the index of the â€œæ”¶åˆ°EZ wayé€šçŸ¥å¾Œâ€ line
    try:
        idx_r = next(i for i,l in enumerate(lines) if l.startswith("æ”¶åˆ°EZ wayé€šçŸ¥å¾Œ"))
    except StopIteration:
        idx_r = len(lines)

    # header before names: up through éº»ç…©è«‹
    header = lines[: idx_m+1 ]

    # footer after names: from æ”¶åˆ°EZ wayé€šçŸ¥å¾Œ onward
    footer = lines[ idx_r: ]

    # collect only the code lines (ACE/250N+name)
    code_lines = [l for l in lines if CODE_TRIGGER_RE.search(l)]

    # strip off the code prefix from each
    cleaned = [ CODE_TRIGGER_RE.sub("", l).strip() for l in code_lines ]
    
    # strip the code prefix and any stray quotes
    cleaned = [
        CODE_TRIGGER_RE.sub("", l).strip().strip('"')
        for l in code_lines
    ]    

    # now split into per-group lists
    vicky_batch = [c for c in cleaned if any(name in c for name in VICKY_NAMES)]
    yumi_batch  = [c for c in cleaned if any(name in c for name in YUMI_NAMES )]

    # extract just the name token (first word) from each cleaned line
    names_only  = [c.split()[0] for c in cleaned]    
    
    # â€œothersâ€ = those whose name token isnâ€™t in any of the three lists
    other_batch = [
        cleaned[i] for i, nm in enumerate(names_only)
        if nm not in VICKY_NAMES
           and nm not in YUMI_NAMES
           and nm not in YVES_NAMES
    ]    

    def push_to(group, batch):
        if not batch:
            # log.info(f"[ACE_SCHEDULE:{label}] batch empty, skipping")
            return
        
        # Build the mini-message: header + blank + batch + blank + footer
        msg_lines = header + [""] + batch + [""] + footer
        text_msg = "\n".join(msg_lines)

        # Push to the group
        requests.post(
            LINE_PUSH_URL,
            headers=LINE_HEADERS,
            json={"to": group, "messages":[{"type":"text","text": text_msg }]}
        )
    
    push_to(VICKY_GROUP_ID, vicky_batch)
    push_to(YUMI_GROUP_ID,  yumi_batch)
    # also push any â€œotherâ€ entries to your personal chat
    push_to(YVES_USER_ID,  other_batch)    

# â”€â”€â”€ Ace shipment-block handler â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def handle_ace_shipments(event):
    """
    Splits the text into blocks starting with 'å‡ºè²¨å–®è™Ÿ:', then
    forwards each complete block to Yumi or Vicky based on the
    recipient name.
    """
    # 1) Grab & clean the raw text
    raw = event["message"]["text"]
    log.info(f"[ACE SHIP] raw incoming text: {repr(raw)}")        # DEBUG log
    text = raw.replace('"', '').strip()                         # strip stray quotes
    
    # split into shipmentâ€blocks
    parts = re.split(r'(?=å‡ºè²¨å–®è™Ÿ:)', text)
    log.info(f"[ACE SHIP] split into {len(parts)} parts")         # DEBUG log
    
    vicky, yumi = [], []

    for blk in parts:
        if "å‡ºè²¨å–®è™Ÿ:" not in blk or "å®…é…å–®è™Ÿ:" not in blk:
            continue
        lines = [l.strip() for l in blk.strip().splitlines() if l.strip()]
        if len(lines) < 4:
            continue
        # recipient name is on line 3
        recipient = lines[2].split()[0]
        full_msg  = "\n".join(lines)
        if recipient in VICKY_NAMES:
            vicky.append(full_msg)
        elif recipient in YUMI_NAMES:
            yumi.append(full_msg)

    def push(group, messages):
        if not messages:
            return
        payload = {
            "to": group,
            "messages":[{"type":"text","text":"\n\n".join(messages)}]
        }
        resp = requests.post(LINE_PUSH_URL, headers=LINE_HEADERS, json=payload)
        log.info(f"Sent {len(messages)} shipment blocks to {group}: {resp.status_code}")

    push(VICKY_GROUP_ID, vicky)
    push(YUMI_GROUP_ID,  yumi)

class LLMAgent:
    def __init__(self):
        api_key = os.getenv("OPENAI_API_KEY")
        self.client = OpenAI(api_key=api_key)

    def inference(self, messages):
        try:
            response = self.client.chat.completions.create(
                model=OPENAI_MODEL,
                messages=messages,
                timeout=30,  # â† prevent worker hangs
            )
            return response.choices[0].message.content.strip()
        except Exception as e:
            log.error(f"[LLM] inference error: {e}")
            return None


# Convert PDF pages to PIL Images using PyMuPDF
def pdf_to_image(pdf_input, dpi=300):
    """
    Convert all pages of a PDF (path or bytes) to a list of PIL Image objects using PyMuPDF.
    """
    # å¦‚æœå‚³å…¥çš„æ˜¯ bytes æˆ– BytesIOï¼Œå°±ç”¨ stream æ¨¡å¼é–‹å•Ÿ
    if isinstance(pdf_input, (bytes, BytesIO)):
        doc = fitz.open(stream=pdf_input, filetype="pdf")
    else:
        doc = fitz.open(pdf_input)
        
    images = []
    # Calculate zoom factor to achieve desired DPI (default is 72)
    zoom = dpi / 72
    matrix = fitz.Matrix(zoom, zoom)

    for page in doc:
        pix = page.get_pixmap(matrix=matrix)
        img = Image.frombytes(
            "RGB",
            [pix.width, pix.height],
            pix.samples
        )
        images.append(img)
        
    # èµ°å®Œæ‰€æœ‰é å¾Œä¸€æ¬¡å›å‚³å®Œæ•´åˆ—è¡¨
    return images  

# Extract text from images using the OpenAI API
def extract_text_from_images(image, prompt="Please extract text from this image."):
    """
    Sends each image to the LLM via base64-encoded data URI and returns a list of responses.
    Also saves each temporary image to disk under temp_images/.
    """
    agent = LLMAgent()
    
    # Barcode (second box): safe crop + decode
    W, H = image.size
    x, y, w, h = 120, 995, 750, 260
    try:
        x1 = max(0, min(x, W))
        y1 = max(0, min(y, H))
        x2 = max(x1, min(x + w, W))
        y2 = max(y1, min(y + h, H))
        tracking_number = ""
        if x2 - x1 > 5 and y2 - y1 > 5:
            cropped_img = image.crop((x1, y1, x2, y2))
            objs = decode(cropped_img)
            if objs:
                try:
                    tracking_number = objs[0].data.decode("utf-8")
                except Exception:
                    tracking_number = ""
    except Exception as _e:
        log.warning(f"[BARCODE] crop/decode failed: {_e}")
        tracking_number = ""

    # Serialize image to JPEG bytes
    buf = BytesIO()
    image.save(buf, format="JPEG")
    img_bytes = buf.getvalue()
    buf.close()

    # Base64 encode
    b64 = base64.b64encode(img_bytes).decode("utf-8")
    data_uri = f"data:image/jpeg;base64,{b64}"

    # Build chat payload
    messages = [
        {
            "role": "user",
            "content": [
                {"type": "text", "text": prompt},
                {"type": "image_url", "image_url": {"url": data_uri}},
            ],
        }
    ]

    # Inference
    text = agent.inference(messages) or "{}"  # ensure non-None
    safe = {"sender": {}, "receiver": {}, "reference number": "", "tracking number": tracking_number}
    try:
        result = json.loads(text)
        if isinstance(result, dict):
            safe.update(result)
        else:
            safe["_raw"] = text
    except json.JSONDecodeError:
        log.error(f"[PDF OCR] JSON parse failed, raw output â†’ {text!r}")
        safe["_raw"] = text
    return safe


# â”€â”€â”€ UPS tracking normalization â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def normalize_ups(trk: str) -> str:
    s = re.sub(r'[^A-Za-z0-9]', '', trk or '').upper()
    if s.startswith('1Z'):
        head, tail = s[:2], s[2:]
        tail = tail.replace('O', '0')  # OCR fix: Oâ†’0 after 1Z
        s = head + tail
    return s

# â”€â”€â”€ lookup_full_tracking å®šç¾© â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def lookup_full_tracking(ups_last4: str) -> Optional[str]:
    """
    åœ¨ Tracking å·¥ä½œè¡¨çš„ S/T/U æ¬„æ‰¾å”¯ä¸€å°¾è™ŸåŒ¹é…ï¼Œå›å‚³å®Œæ•´è¿½è¹¤ç¢¼æˆ– Noneã€‚
    """
    SHEET_ID = "1BgmCA1DSotteYMZgAvYKiTRWEAfhoh7zK9oPaTTyt9Q"
    gs = get_gspread_client()
    ss = gs.open_by_key(SHEET_ID)
    ws = ss.worksheet("Tracking")

    cols = [19, 20, 21]  # S=19, T=20, U=21
    matches = []
    pat = re.compile(r"[A-Za-z0-9]{18}")  # typical UPS length
    for col_idx in cols:
        for v in ws.col_values(col_idx)[1:]:
            v = (v or "").strip().upper()
            if pat.fullmatch(v) and v.endswith(ups_last4):
                matches.append(v)
            if len(matches) > 1:
                break

    if len(matches) != 1:
        log.warning(f"UPSå°¾è™Ÿ {ups_last4} æ‰¾åˆ° {len(matches)} ç­†ï¼Œä¸å”¯ä¸€ï¼Œè·³é")
        return None
    return matches[0]

#Add a forgiving JSON parser
def parse_json_forgiving(s):
    """
    Accepts a dict OR a JSON-ish string (may contain ```json fences or stray text).
    Returns {} on failure.
    """
    if isinstance(s, dict):
        return s
    if not isinstance(s, str):
        return {}
    txt = s.strip()
    # strip ```json fences if present
    if txt.startswith("```"):
        txt = txt.strip("`")
        # remove possible leading 'json'
        if txt.lower().startswith("json"):
            txt = txt[4:]
    # remove any leading/trailing code fences/newlines
    txt = txt.strip()
    try:
        return json.loads(txt)
    except Exception:
        return {}


# CLI entrypoint
def main():
    pdf_path = "U110252577.pdf"
    dpi = 300
    prompt = OCR_SHIPPING_PROMPT

    # Convert and extract
    images = pdf_to_image(pdf_path, dpi=dpi)
    results = [extract_text_from_images(img, prompt=prompt) for img in images]
    print(text)

# â”€â”€â”€ Flask Webhook â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
app = Flask(__name__)

@app.route("/webhook", methods=["GET", "POST"])
def webhook():

    import re
    # Log incoming methods
    # print(f"[Webhook] Received {request.method} to /webhook")
    # log.info(f"Received {request.method} to /webhook")
    if request.method == "GET":
        return "OK", 200

    data = request.get_json()
    print("[Webhook] Payload:", json.dumps(data, ensure_ascii=False))
    # log.info(f"Payload: {json.dumps(data, ensure_ascii=False)}")

    for event in (data or {}).get("events", []):
        # ignore nonâ€message events (eg. unsend)
        if event.get("type") != "message":
            continue
            
        # ç«‹åˆ»æŠ“ source / group_id
        src = event.get("source") or {}
        group_id = src.get("groupId")
        msg = event.get("message") or {}
        text     = msg.get("text", "").strip()
        mtype    = msg.get("type")
    
        # â”€â”€â”€ PDF OCR trigger (for multiple allowed groups) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        if (
            msg.get("type") == "file"
            and msg.get("fileName", "").lower().endswith(".pdf")
            and src.get("type") == "group"
            and src.get("groupId") in {
                VICKY_GROUP_ID,
                YUMI_GROUP_ID,
                JOYCE_GROUP_ID,
                PDF_GROUP_ID,
            }
        ):
            file_id = msg["id"]
            original_filename = msg.get("fileName", "uploaded.pdf") # fallback æª”å

            # >>> ASYNC PDF OCR (enqueue & immediate ACK) >>>
            reply_token = event.get("replyToken")

            # Deduplicate by file_id (LINE may retry same event)
            with _EVENT_DEDUP_LOCK:
                if file_id in _EVENT_DEDUP:
                    log.info(f"[PDF OCR] duplicate event for file_id={file_id}, skipping enqueue")
                else:
                    _EVENT_DEDUP.add(file_id)
                    EXECUTOR.submit(
                        process_pdf_ocr_job,
                        file_id=file_id,
                        group_id=group_id,
                        original_filename=original_filename,
                    )
                    log.info(f"[PDF OCR] enqueued job for {original_filename} ({file_id}) to group {group_id}")

            # Best-effort short 'processing' reply (safe to ignore failures)
            try:
                if reply_token:
                    requests.post(
                        LINE_REPLY_URL,
                        headers=LINE_HEADERS,
                        json={
                            "replyToken": reply_token,
                            "messages": [
                                {"type": "text", "text": f"ğŸ§¾ å·²æ”¶åˆ° {original_filename}ï¼Œæ­£åœ¨é€²è¡Œ OCRâ€¦"}
                            ],
                        },
                        timeout=10,
                    )
            except Exception as e:
                log.warning(f"[PDF OCR] reply (processing notice) failed: {e}")

            # Important: skip the old (blocking) PDF logic below
            continue
            # <<< ASYNC PDF OCR <<<
 
        # â”€â”€â”€ If image, run ONLY the barcode logic and then continue â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        if mtype == "image":
            is_from_me      = src.get("type") == "user"  and src.get("userId")  == YVES_USER_ID
            is_from_ace     = src.get("type") == "group" and src.get("groupId") == ACE_GROUP_ID
            is_from_soquick = src.get("type") == "group" and src.get("groupId") == SOQUICK_GROUP_ID
            if not (is_from_me or is_from_ace or is_from_soquick):
                continue

            try:
                # (1) Download raw image bytes from LINE
                message_id = event["message"]["id"]
                stream_resp = requests.get(
                    f"https://api-data.line.me/v2/bot/message/{message_id}/content",
                    headers={"Authorization": f"Bearer {LINE_TOKEN}"},
                    stream=True
                )
                stream_resp.raise_for_status()
                chunks = []
                for chunk in stream_resp.iter_content(chunk_size=4096):
                    if chunk:
                        chunks.append(chunk)
                raw_bytes = b"".join(chunks)
                # log.info(f"[OCR] Downloaded {len(raw_bytes)} bytes from LINE")
                log.info(f"[BARCODE] Downloaded {len(raw_bytes)} bytes from LINE")

                # (2) Load into Pillow and autoâ€crop to dark (text/barcode) region
                img = Image.open(io.BytesIO(raw_bytes)).convert("RGB")
                                
                # â”€â”€ DEBUG CHANGE: use full-resolution image, no thumbnail â”€â”€
                img_crop = img
                log.info(f"[BARCODE] Decoding fullâ€resolution image size {img_crop.size}")

                # (4) Decode any barcodes in the PIL image
                # Instead of decoding only CODE128, we now include multiple symbologies:
                decoded_objs = decode(
                    img_crop,
                    symbols=[ZBarSymbol.CODE128, ZBarSymbol.CODE39, ZBarSymbol.EAN13, ZBarSymbol.UPCA]
                )

                if not decoded_objs:
                    log.info("[BARCODE] No barcode detected in the image.")
                    # reply_payload = {
                        # "replyToken": event["replyToken"],
                        # "messages": [
                            # {
                                # "type": "text",
                                # "text": "No barcode detected. Please try again with a clearer image."
                            # }
                        # ]
                    # }
                    # requests.post(
                        # "https://api.line.me/v2/bot/message/reply",
                        # headers={
                            # "Content-Type": "application/json",
                            # "Authorization": f"Bearer {LINE_TOKEN}"
                        # },
                        # json=reply_payload
                    # )
                else:
                    # 1. Take the first decoded barcode as the Tracking ID
                    for obj in decoded_objs:
                        log.info(f"[BARCODE] Detected: {obj.type} â†’ {obj.data.decode('utf-8')}")
                    tracking_raw = next(
                        (obj.data.decode("utf-8") for obj in decoded_objs if obj.data.decode("utf-8").startswith("1Z")),
                        decoded_objs[0].data.decode("utf-8")  # fallback
                    )

                    log.info(f"[BARCODE] First decoded raw data (tracking): {tracking_raw}")

                    # 2. If there is a tracking ID (we already decode it)
                    # tracking_id = decoded_objs[0].data.decode("utf-8").strip()
                    tracking_id = tracking_raw.strip()
                    log.info(f"[BARCODE] Decoded tracking ID: {tracking_id}")

                    # â”€â”€â”€ Lookup the subitem directly on the subitem board via items_page_by_column_values â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
                    q_search = """
                    query (
                      $boardId: ID!
                      $columnId: String!
                      $value: String!
                    ) {
                      items_page_by_column_values(
                        board_id: $boardId,
                        limit: 1,
                        columns: [
                          { column_id: $columnId, column_values: [$value] }
                        ]
                      ) {
                        items {
                          id
                          name
                        }
                      }
                    }
                    """
                    vars_search = {
                      "boardId":  os.getenv("AIR_BOARD_ID"),  # must be your subitemâ€board ID
                      "columnId": "name",
                      "value":    tracking_id
                    }
                    r_search = requests.post(
                      "https://api.monday.com/v2",
                      headers={
                        "Authorization": MONDAY_API_TOKEN,
                        "Content-Type":  "application/json"
                      },
                      json={ "query": q_search, "variables": vars_search }
                    )
                    if r_search.status_code != 200:
                        log.error("[MONDAY] search failed %s: %s", r_search.status_code, r_search.text)
                        continue

                    items_page = r_search.json().get("data", {}) \
                                          .get("items_page_by_column_values", {}) \
                                          .get("items", [])
                    if not items_page:
                        log.warning(f"Tracking ID {tracking_id} not found in subitem board")
                        requests.post(
                          LINE_PUSH_URL, headers=LINE_HEADERS,
                          json={
                            "to": YVES_USER_ID,
                            "messages": [
                              {
                                "type": "text",
                                "text": f"âš ï¸ Tracking ID {tracking_id} not found in Monday."
                              }
                            ]
                          }
                        )
                        continue

                    found_subitem_id = items_page[0]["id"]
                    log.info(f"Found subitem {found_subitem_id} for {tracking_id}")
                                 
                    # STORE for next text event
                    pending_key = f"last_subitem_for_{group_id}"
                    r.set(pending_key, found_subitem_id, ex=300)
                    log.info(f"Stored subitem ID {found_subitem_id} for next text parsing (group {group_id})")
                    # â”€â”€ END STORE â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
###
                    # first decide location text based on which group this came from
                    src = event.get("source", {})

                    if group_id == ACE_GROUP_ID:
                        loc = "æº«å“¥è¯å€‰A"
                    elif group_id == SOQUICK_GROUP_ID:
                        loc = "æº«å“¥è¯å€‰S"
                    else:
                        # fallback or skip summary tracking if you prefer
                        loc = "Yves/Simply"

                    # â”€â”€â”€ Update Location & Status â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
                    mutation = """
                    mutation ($itemId: ID!, $boardId: ID!, $columnVals: JSON!) {
                      change_multiple_column_values(
                        item_id: $itemId,
                        board_id: $boardId,
                        column_values: $columnVals
                      ) { id }
                    }
                    """
                    variables = {
                      "itemId":    found_subitem_id,
                      "boardId":   os.getenv("AIR_BOARD_ID"),  # same subitemâ€board
                      "columnVals": json.dumps({
                        "location__1": { "label": loc },
                        "status__1":    { "label": "æ¸¬é‡" }
                      })
                    }
                    up = requests.post(
                      "https://api.monday.com/v2",
                      headers={
                        "Authorization": MONDAY_API_TOKEN,
                        "Content-Type":  "application/json"
                      },
                      json={ "query": mutation, "variables": variables }
                    )
                    if up.status_code != 200:
                        log.error("[MONDAY] update failed %s: %s", up.status_code, up.text)
                    else:
                        log.info(f"Updated subitem {found_subitem_id}: location & status set")

                        # â”€â”€â”€ BATCH SUMMARY TRACKING â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
                        _pending[group_id].append(tracking_id)
                        if group_id not in _scheduled:
                            _scheduled.add(group_id)
                            # schedule the summary for this group in 30 minutes
                            threading.Timer(30*60, _schedule_summary, args=[group_id]).start()

                    # 3. If there is a second decoded value, extract the postal code portion
                    if len(decoded_objs) > 1:
                        postal_raw = decoded_objs[1].data.decode("utf-8")  # e.g. "420V6X1Z7"
                        # Extract everything after the first three characters:
                        postal_code = postal_raw[3:]  # yields "V6X1Z7"
                        log.info(f"[BARCODE] Extracted postal code (not printed): {postal_code}")

                        # 4. Save postal_code into memory (bio)
                        #    This call uses the 'bio' tool so that future conversations can recall it.
                        #    We do not print it to the user now.
                        # 
                        # Format: just the fact we want to remember, e.g. "Postal code V6X1Z7"
                        #
                        # (A separate tool call below will persist this memory.)

                        # â—† â—† â—† Tool call follows below â—† â—† â—†

            except Exception:
                # Log any barcode or Monday API errors without replying to the chat
                log.error("[BARCODE] Error during image handling", exc_info=True)
                # log.error("[BARCODE] Error decoding barcode", exc_info=True)
                # Optionally, reply â€œNONEâ€ or a helpful message:
                # error_payload = {
                    # "replyToken": event["replyToken"],
                    # "messages": [
                        # {
                            # "type": "text",
                            # "text": "An error occurred while reading the image. Please try again."
                        # }
                    # ]
                # }
                # requests.post(
                    # "https://api.line.me/v2/bot/message/reply",
                    # headers={
                        # "Content-Type": "application/json",
                        # "Authorization": f"Bearer {LINE_TOKEN}"
                    # },
                    # json=error_payload
                # )
            # now that images are handled, skip text logic
            continue
    
        # 0) åªè™•ç†æ–‡å­—
        if mtype != "text":
            continue
        
        # 1) å¤šç­† UPS æœ«å››ç¢¼ï¼‹é‡é‡ï¼‹å°ºå¯¸ ä¸€æ¬¡è™•ç†
        # åŒæ™‚æ”¯æ´ã€Œ*ã€ã€ŒÃ—ã€ã€Œxã€æˆ–ã€Œç©ºç™½ã€åˆ†éš”
        multi_pat = re.compile(
            r'(\d{4})\s+'             # 4ä½UPSå°¾è™Ÿ
            r'([\d.]+)kg\s+'          # é‡é‡ (kg)
            r'(\d+)'                  # å¯¬
            r'(?:[Ã—x*\s]+)'           # å…è¨± Ã— x * æˆ–ç©ºç™½ ä½œç‚ºåˆ†éš”
            r'(\d+)'                  # é«˜
            r'(?:[Ã—x*\s]+)'           # å†æ¬¡å…è¨±å„ç¨®åˆ†éš”
            r'(\d+)'                  # æ·±
            r'(?:cm)?',               # å¯é¸çš„ã€Œcmã€
            re.IGNORECASE
        )
        matches = multi_pat.findall(text)  # æ‰¾å‡ºæ‰€æœ‰ç¬¦åˆæ ¼å¼çš„ tuple åˆ—è¡¨

        if matches:
            for ups4, wt_str, w, h, d in matches:
                # â€”(1) å¾ Google Sheets æ‰¾å›å®Œæ•´è¿½è¹¤ç¢¼
                full_no = lookup_full_tracking(ups4)
                if not full_no:
                    # å¦‚æœæ‰¾ä¸åˆ°æˆ–ä¸å”¯ä¸€ï¼Œè·³éæœ¬ç­†
                    continue

                # â€”(2) è§£æé‡é‡èˆ‡å°ºå¯¸
                weight_kg = float(wt_str)      # å°‡å­—ä¸²è½‰ç‚º float
                dims_norm = f"{w}*{h}*{d}"    # çµ„æˆ "é•·*å¯¬*é«˜" å­—ä¸²

                # â€”(3) ç”¨å®Œæ•´è¿½è¹¤ç¢¼åˆ° Monday æŸ¥ subitem (Name æ¬„)
                find_q = f'''
                query {{
                  items_by_column_values(
                    board_id: {os.getenv("AIR_BOARD_ID")},
                    column_id: "name",
                    column_value: "{full_no}"
                  ) {{ id }}
                }}'''
                resp = requests.post(
                    "https://api.monday.com/v2",
                    headers={ "Authorization": MONDAY_API_TOKEN,
                              "Content-Type":  "application/json" },
                    json={ "query": find_q }
                )
                items = resp.json().get("data", {}) \
                                 .get("items_by_column_values", [])
                if not items:
                    log.warning(f"Monday: subitem åç¨±={full_no} æ‰¾ä¸åˆ°ï¼Œè·³é")
                    continue

                sub_id = items[0]["id"]  # å–ç¬¬ä¸€å€‹ match çš„ subitem ID

                # â€”(4) ä¸Šå‚³å°ºå¯¸ (__1__cm__1 æ¬„)
                dim_mut = f'''
                mutation {{
                  change_simple_column_value(
                    item_id: {sub_id},
                    board_id: {os.getenv("AIR_BOARD_ID")},
                    column_id: "__1__cm__1",
                    value: "{dims_norm}"
                  ) {{ id }}
                }}'''
                requests.post(
                    "https://api.monday.com/v2",
                    headers={ "Authorization": MONDAY_API_TOKEN,
                              "Content-Type":  "application/json" },
                    json={ "query": dim_mut }
                )

                # â€”(5) ä¸Šå‚³é‡é‡ (numeric__1 æ¬„)
                wt_mut = f'''
                mutation {{
                  change_simple_column_value(
                    item_id: {sub_id},
                    board_id: {os.getenv("AIR_BOARD_ID")},
                    column_id: "numeric__1",
                    value: "{weight_kg:.2f}"
                  ) {{ id }}
                }}'''
                requests.post(
                    "https://api.monday.com/v2",
                    headers={ "Authorization": MONDAY_API_TOKEN,
                              "Content-Type":  "application/json" },
                    json={ "query": wt_mut }
                )

                # â€”(6) ç¿»è½‰ç‹€æ…‹åˆ°ã€Œæº«å“¥è¯æ”¶æ¬¾ã€(status__1 æ¬„)
                stat_mut = f'''
                mutation {{
                  change_simple_column_value(
                    item_id: {sub_id},
                    board_id: {os.getenv("AIR_BOARD_ID")},
                    column_id: "status__1",
                    value: "{{\\"label\\":\\"æº«å“¥è¯æ”¶æ¬¾\\"}}"
                  ) {{ id }}
                }}'''
                requests.post(
                    "https://api.monday.com/v2",
                    headers={ "Authorization": MONDAY_API_TOKEN,
                              "Content-Type":  "application/json" },
                    json={ "query": stat_mut }
                )

                # â€”(7) æ—¥èªŒï¼šç¢ºèªæ›´æ–°å®Œç•¢
                log.info(f"[UPSâ†’Monday] {full_no} æ›´æ–°: é‡é‡={weight_kg}kg, å°ºå¯¸={dims_norm}")

            # è™•ç†å®Œæ‰€æœ‰å¤šç­† UPS å¾Œï¼Œè·³éå¾ŒçºŒä»»ä½• handler
            continue

        # 2) pending_key å–®ç­† size/weight parser
        pending_key = f"last_subitem_for_{group_id}"
        sub_id = r.get(pending_key)
        if sub_id:
            size_text = text
            log.info(f"Parsing size_text for subitem {sub_id!r}: {size_text!r}")

            # parse weight
            wm = re.search(r"(\d+(?:\.\d+)?)\s*(kg|å…¬æ–¤|lbs?)", size_text, re.IGNORECASE)
            if wm:
                qty, unit = float(wm.group(1)), wm.group(2).lower()
                weight_kg = qty * (0.453592 if unit.startswith("lb") else 1.0)
                log.info(f"  â†’ Parsed weight_kg: {weight_kg:.2f} kg")
            else:
                weight_kg = None

            # parse dimensions
            dm = re.search(
              # allow Ã—, x, *, or any whitespace between numbers
              r"(\d+(?:\.\d+)?)[Ã—x*\s]+(\d+(?:\.\d+)?)[Ã—x*\s]+(\d+(?:\.\d+)?)(?:\s*)(cm|å…¬åˆ†|in|å‹)?",
              size_text, re.IGNORECASE
            )
            if dm:
                # capture groups: 1=width, 2=height, 3=depth, 4=unit (optional)
                w, h, d = map(float, dm.group(1,2,3))
                unit = (dm.group(4) or "cm").lower()
                factor = 2.54 if unit.startswith(("in","å‹")) else 1.0
                # use '*' between numbers, always
                dims_norm = f"{int(w*factor)}*{int(h*factor)}*{int(d*factor)}"
                log.info(f"  â†’ Parsed dims_norm: {dims_norm}")
            else:
                dims_norm = None
                log.debug("  â†’ No dimensions match")

            # helper to build the mutation
            def mutate(colId, val):
                return f'''
                mutation {{
                  change_simple_column_value(
                    item_id: {sub_id},
                    board_id: {os.getenv("AIR_BOARD_ID")},
                    column_id: "{colId}",
                    value: "{val}"
                  ) {{ id }}
                }}'''

            # push dimensions if found
            if dims_norm:
                requests.post(
                  "https://api.monday.com/v2",
                  headers={ "Authorization": MONDAY_API_TOKEN, "Content-Type": "application/json" },
                  json={ "query": mutate("__1__cm__1", dims_norm) }
                )

            # push weight if found
            if weight_kg is not None:
                requests.post(
                  "https://api.monday.com/v2",
                  headers={ "Authorization": MONDAY_API_TOKEN, "Content-Type": "application/json" },
                  json={ "query": mutate("numeric__1", f"{weight_kg:.2f}") }
                )
                
            # now that we got weight, clear pending so we don't parse again
            r.delete(pending_key)
            log.info(f"Cleared pending for subitem {sub_id}")

            # â”€â”€ if dims+weight and status is â€œæ¸¬é‡â€, bump to â€œæº«å“¥è¯æ”¶æ¬¾â€ â”€â”€â”€â”€â”€
            if dims_norm is not None and weight_kg is not None:
                status_mut = f'''
                mutation {{
                  change_column_value(
                    item_id: {sub_id},
                    board_id: {os.getenv("AIR_BOARD_ID")},
                    column_id: "status__1",
                    value: "{{\\"label\\":\\"æº«å“¥è¯æ”¶æ¬¾\\"}}"
                  ) {{ id }}
                }}'''
                resp = requests.post(
                  "https://api.monday.com/v2",
                  headers={
                    "Authorization": MONDAY_API_TOKEN,
                    "Content-Type":  "application/json"
                  },
                  json={ "query": status_mut }
                )
                if resp.status_code == 200:
                    log.info(f"Updated status to æº«å“¥è¯æ”¶æ¬¾ for subitem {sub_id}")
                else:
                    log.error(f"Failed to update status for subitem {sub_id}: {resp.text}")

            # whether dims or weight or both, log final
            log.info(f"Finished size/weight sync for subitem {sub_id}: dims={dims_norm!r}, weight={weight_kg!r}")
            continue
 
        # 3) Ace schedule (é€±å››ï¼é€±æ—¥å‡ºè²¨) & ACE EZ-Way check
        if group_id == ACE_GROUP_ID and ("é€±å››å‡ºè²¨" in text or "é€±æ—¥å‡ºè²¨" in text):
            handle_ace_schedule(event)
            handle_ace_ezway_check_and_push_to_yves(event)
            continue

        # 4) è™•ç†ã€Œç”³å ±ç›¸ç¬¦ã€æé†’
        if "ç”³å ±ç›¸ç¬¦" in text and CODE_TRIGGER_RE.search(text):
            handle_missing_confirm(event)
            continue
        
        # 5) Richmond-arrival triggers content-request to Vicky â€”â€”â€”â€”â€”â€”â€”â€”â€”
        if group_id == VICKY_GROUP_ID and "[Richmond, Canada] å·²åˆ°é”æ´¾é€ä¸­å¿ƒ" in text:
            # extract the tracking ID inside parentheses
            import re
            m = re.search(r"\(([^)]+)\)", text)
            if m:
                tracking_id = m.group(1)
            else:
                # no ID found, skip
                continue

            # build the mention message
            placeholder = "{user1}"
            msg = f"{placeholder} è«‹æä¾›æ­¤åŒ…è£¹çš„å…§å®¹ç‰©æ¸…å–®ï¼š{tracking_id}"
            substitution = {
                "user1": {
                    "type": "mention",
                    "mentionee": {
                        "type":   "user",
                        "userId": VICKY_USER_ID
                    }
                }
            }
            payload = {
                "to": VICKY_GROUP_ID,
                "messages": [{
                    "type":        "textV2",
                    "text":        msg,
                    "substitution": substitution
                }]
            }
            requests.post(LINE_PUSH_URL, headers=LINE_HEADERS, json=payload)
            log.info(f"Requested contents list from Vicky for {tracking_id}")
            continue
                
        # 6) Soquick â€œä¸Šå‘¨å…­å‡ºè²¨åŒ…è£¹çš„æ´¾ä»¶å–®è™Ÿâ€ & Ace "å‡ºè²¨å–®è™Ÿ" blocks â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”
        if (group_id == SOQUICK_GROUP_ID and "ä¸Šå‘¨å…­å‡ºè²¨åŒ…è£¹çš„æ´¾ä»¶å–®è™Ÿ" in text) or (group_id == ACE_GROUP_ID and "å‡ºè²¨å–®è™Ÿ" in text and "å®…é…å–®è™Ÿ" in text):
            handle_soquick_and_ace_shipments(event)
            continue

        # 7) Soquick â€œè«‹é€šçŸ¥â€¦ç”³å ±ç›¸ç¬¦â€ messages â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”
        log.info(
            "[SOQ DEBUG] group_id=%r, SOQUICK_GROUP_ID=%r, "
            "has_æ‚¨å¥½=%r, has_æŒ‰=%r, has_ç”³å ±ç›¸ç¬¦=%r",
            group_id,
            SOQUICK_GROUP_ID,
            "æ‚¨å¥½ï¼Œè«‹" in text,
            "æŒ‰" in text,
            "ç”³å ±ç›¸ç¬¦" in text,
        )        
        if (group_id == SOQUICK_GROUP_ID
            and "æ‚¨å¥½ï¼Œè«‹" in text
            and "æŒ‰" in text
            and "ç”³å ±ç›¸ç¬¦" in text):
            handle_soquick_full_notification(event)
            continue          

        # 8) Your existing â€œè¿½è¹¤åŒ…è£¹â€ logic
        if text == "è¿½è¹¤åŒ…è£¹":
            keywords = CUSTOMER_FILTERS.get(group_id)
            if not keywords:
                print(f"[Webhook] No keywords configured for group {group_id}, skipping.")
                continue

            # Now safe to extract reply_token
            reply_token = event["replyToken"]
            print("[Webhook] Trigger matched, fetching statusesâ€¦")
            messages = get_statuses_for(keywords)
            print("[Webhook] Reply messages:", messages)

            # Combine lines into one multi-line text
            combined = "\n\n".join(messages)
            payload = {
                "replyToken": reply_token,
                "messages": [{"type": "text", "text": combined}]
            }

            headers = {
                "Content-Type": "application/json",
                "Authorization": f"Bearer {LINE_TOKEN}"
            }
            resp = requests.post(
                "https://api.line.me/v2/bot/message/reply",
                headers=headers,
                json=payload
            )
            print(f"[Webhook] LINE reply status: {resp.status_code}, body: {resp.text}")
            log.info(f"LINE reply status={resp.status_code}, body={resp.text}")

        # 9) Your existing â€œä¸‹å€‹åœ‹å®šå‡æ—¥â€ logic
        if text == "ä¸‹å€‹åœ‹å®šå‡æ—¥":
            from holiday_reminder import get_next_holiday
            msg = get_next_holiday()
            reply_token = event["replyToken"]
            requests.post(
                "https://api.line.me/v2/bot/message/reply",
                headers={
                    "Authorization": f"Bearer {LINE_TOKEN}",
                    "Content-Type": "application/json"
                },
                json={
                    "replyToken": reply_token,
                    "messages": [{"type": "text", "text": msg}]
                }
            )

        # ğŸŸ¢ NEW: ACE manual trigger â€œå·²ä¸Šå‚³è³‡æ–™å¯å‡ºè²¨â€
        if (
            event.get("source", {}).get("type") == "group"
            and event["source"].get("groupId") == ACE_GROUP_ID
            and text.strip() == "å·²ä¸Šå‚³è³‡æ–™å¯å‡ºè²¨"
        ):
            reply_token = event.get("replyToken")
            push_ace_today_shipments(force=True, reply_token=reply_token)
            return "OK", 200

    return "OK", 200
    
# â”€â”€â”€ Monday.com Webhook â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
@app.route("/monday-webhook", methods=["GET", "POST"])
def monday_webhook():
    if request.method == "GET":
        return "OK", 200

    data = request.get_json()
    evt  = data.get("event", data)
    # respond to Mondayâ€™s handshake
    if "challenge" in data:
        return jsonify({"challenge": data["challenge"]}), 200

    sub_id    = evt.get("pulseId") or evt.get("itemId")
    parent_id = evt.get("parentItemId")
    lookup_id = parent_id or sub_id
    new_txt   = evt.get("value", {}).get("label", {}).get("text")

    # only act when Location flips to åœ‹éš›é‹è¼¸
    if new_txt != "åœ‹éš›é‹è¼¸" or not lookup_id:
        return "OK", 200

    # fetch just the formula column:
    gql = '''
    query ($itemIds: [ID!]!) {
      items(ids: $itemIds) {
        column_values(ids: ["formula8__1"]) {
          id
          text
          ... on FormulaValue { display_value }
        }
      }
    }'''
    variables = {"itemIds": [str(lookup_id)]}
    resp = requests.post(
      "https://api.monday.com/v2",
      json={"query": gql, "variables": variables},
      headers={
        "Authorization": MONDAY_API_TOKEN,
        "Content-Type":  "application/json"
      }
    )
    data2 = resp.json()

    # grab that single column_value
    cv = data2["data"]["items"][0]["column_values"][0]
    client = (cv.get("text") or cv.get("display_value") or "").strip()
    key    = client.lower()     # e.g. "yumi" or "vicky"

    group_id = CLIENT_TO_GROUP.get(key)
    if not group_id:
        print(f"[Mondayâ†’LINE] no mapping for â€œ{client}â€ â†’ {key}, skipping.")
        log.warning(f"No mapping for client={client} key={key}, skipping.")
        return "OK", 200

    item_name = evt.get("pulseName") or str(lookup_id)
    message   = f"ğŸ“¦ {item_name} å·²é€å¾€æ©Ÿå ´ï¼Œæº–å‚™é€²è¡Œåœ‹éš›é‹è¼¸ã€‚"

    push = requests.post(
      "https://api.line.me/v2/bot/message/push",
      headers={
        "Authorization": f"Bearer {LINE_TOKEN}",
        "Content-Type":  "application/json"
      },
      json={"to": group_id, "messages":[{"type":"text","text":message}]}
    )
    print(f"[Mondayâ†’LINE] sent to {client}: {push.status_code}", push.text)
    log.info(f"Mondayâ†’LINE push status={push.status_code}, body={push.text}")

    return "OK", 200
 
# â”€â”€â”€ Poller State Helpers & Job â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# â”€â”€â”€ Helpers for parsing batch lines â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def extract_order_key(line: str) -> str:
    return line.rsplit("@",1)[0].strip()

def extract_timestamp(line: str) -> str:
    return line.rsplit("@",1)[1].strip()

def load_state():
    """Fetch the JSON-encoded map of order_keyâ†’timestamp from Redis."""
    data = r.get("last_seen")
    return json.loads(data) if data else {}

def save_state(state):
    """Persist the map of order_keyâ†’timestamp back to Redis."""
    r.set("last_seen", json.dumps(state))

def check_te_updates():
    """Poll TE API every interval; push only newly changed statuses."""
    state = load_state()
    for group_id, keywords in CUSTOMER_FILTERS.items():
        lines = get_statuses_for(keywords)
        new_lines = []
        for line in lines[1:]:
            ts = extract_timestamp(line)
            key = extract_order_key(line)
            if state.get(key) != ts:
                state[key] = ts
                new_lines.append(line)
        if new_lines:
            payload = {
                "to": group_id,
                "messages": [{
                    "type": "text",
                    "text": "\n\n".join(new_lines)
                }]
            }
            requests.post(LINE_PUSH_URL, headers=LINE_HEADERS, json=payload)
    save_state(state)   

##â€”â€”â€” Vicky reminders (Wed & Fri at 18:00) â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”
# sched.add_job(lambda: remind_vicky("æ˜ŸæœŸå››"),
              # trigger="cron", day_of_week="wed", hour=18, minute=00)
# sched.add_job(lambda: remind_vicky("é€±æœ«"),
              # trigger="cron", day_of_week="fri", hour=17, minute=00)

# sched.start()
# log.info("Scheduler started")

if __name__ == "__main__":
    app.run(host="0.0.0.0", port=int(os.getenv("PORT", 5000)))