import os
import requests
import json
import redis
import logging
import re
import threading
from flask import Flask, request, jsonify
from datetime import datetime, timezone, timedelta
import pytz
import openai

# åŸºç¤é…ç½®èˆ‡å·¥å…·
import config
from redis_client import r
from log import log

# æ ¸å¿ƒæœå‹™å±¤
from services.ocr_engine import OCRAgent
from services.monday_service import MondaySyncService
from services.te_api_service import get_statuses_for, call_api
from services.barcode_service import handle_barcode_image
from services.twws_service import get_twws_value_by_name
from services.shipment_parser import ShipmentParserService

# æ¥­å‹™é‚è¼¯è™•ç†å™¨
from handlers.handlers import (
    handle_ace_ezway_check_and_push_to_yves,
    handle_soquick_and_ace_shipments,
    handle_ace_shipments,
    handle_ace_schedule,
    handle_missing_confirm,
    handle_soquick_full_notification
)
from handlers.unpaid_handler import handle_unpaid_event
from handlers.vicky_handler import remind_vicky

# å·¥ä½œæ’ç¨‹
from jobs.ace_tasks import push_ace_today_shipments
from jobs.sq_tasks import push_sq_weekly_shipments

from sheets import get_gspread_client

from collections import defaultdict
from typing import Optional, List, Dict, Any


# â”€â”€â”€ Client â†’ LINE Group Mapping â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
CLIENT_TO_GROUP = {
    "yumi":  os.getenv("LINE_GROUP_ID_YUMI"),
    "vicky": os.getenv("LINE_GROUP_ID_VICKY"),
}

# â”€â”€â”€ Environment Variables â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
APP_ID      = os.getenv("TE_APP_ID")          # e.g. "584"
APP_SECRET  = os.getenv("TE_SECRET")          # your TE App Secret

# â”€â”€â”€ LINE & ACE/SQ è¨­å®š â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ACE_GROUP_ID     = os.getenv("LINE_GROUP_ID_ACE")
GORSKY_USER_ID   = os.getenv("GORSKY_USER_ID")
SOQUICK_GROUP_ID = os.getenv("LINE_GROUP_ID_SQ")
VICKY_GROUP_ID   = os.getenv("LINE_GROUP_ID_VICKY")
VICKY_USER_ID    = os.getenv("VICKY_USER_ID") 
YVES_USER_ID     = os.getenv("YVES_USER_ID") 
YUMI_GROUP_ID    = os.getenv("LINE_GROUP_ID_YUMI")
JOYCE_GROUP_ID   = os.getenv("LINE_GROUP_ID_JOYCE")
IRIS_GROUP_ID    = os.getenv("LINE_GROUP_ID_IRIS")
PDF_GROUP_ID     = os.getenv("LINE_GROUP_ID_PDF")

SQ_SHEET_URL     = os.getenv("SQ_SHEET_URL")
ACE_SHEET_URL = os.getenv("ACE_SHEET_URL")

# --- Timezone (used by schedulers) ---
TIMEZONE = os.getenv("TIMEZONE", "America/Vancouver")

# Trigger when you see â€œé€±å››å‡ºè²¨â€/â€œé€±æ—¥å‡ºè²¨â€ + â€œéº»ç…©è«‹â€ + an ACE or 250N code,
# or when you see the exact phrase â€œé€™å¹¾ä½é‚„æ²’æœ‰æŒ‰ç”³å ±ç›¸ç¬¦â€
CODE_TRIGGER_RE = re.compile(r"\b(?:ACE|\d+N)\d*[A-Z0-9]*\b")
MISSING_CONFIRM = "é€™å¹¾ä½é‚„æ²’æœ‰æŒ‰ç”³å ±ç›¸ç¬¦"

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ACEã€Œä»Šæ—¥å‡ºè²¨ã€ï¼šæ’ç¨‹ + æ‰‹å‹•è§¸ç™¼
# - ä¾†æºï¼šACE_SHEET_URL æŒ‡å‘çš„ Google Sheet
# - è¦å‰‡ï¼šæ‰¾å‡ºã€Œä»Šå¤©ã€åœ¨æ¬„ A çš„æ‰€æœ‰åˆ— â†’ å–è©²åˆ—çš„æ¬„ Bï¼ˆBox IDï¼‰ï¼Œçµ„æˆè¨Šæ¯æ¨æ’­
# - æ™‚é–“ï¼šæ¯é€±å››ã€é€±æ—¥ä¸‹åˆ 4:00ï¼ˆAmerica/Vancouverï¼‰
# - æ‰‹å‹•ï¼šåœ¨ ACE ç¾¤çµ„è¼¸å…¥ã€Œå·²ä¸Šå‚³è³‡æ–™å¯å‡ºè²¨ã€ç«‹å³è§¸ç™¼ï¼ˆä¸å—æ¯æ—¥é˜²é‡è¤‡é™åˆ¶ï¼‰
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

# â”€â”€â”€ Redis for state persistence â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
REDIS_URL = os.getenv("REDIS_URL")
if not REDIS_URL:
    raise RuntimeError("REDIS_URL environment variable is required for state persistence")
r = redis.from_url(REDIS_URL, decode_responses=True)

# pull your sheet URL / ID from env
VICKY_SHEET_URL = os.getenv("VICKY_SHEET_URL")

MONDAY_API_TOKEN = os.getenv("MONDAY_API_TOKEN")

AIR_BOARD_ID = os.getenv("AIR_BOARD_ID")
AIR_PARENT_BOARD_ID = os.getenv("AIR_PARENT_BOARD_ID")

#STATE_FILE = os.getenv("STATE_FILE", "last_seen.json")
LINE_PUSH_URL = "https://api.line.me/v2/bot/message/push"
LINE_HEADERS = {
    "Content-Type":  "application/json",
    "Authorization": f"Bearer {config.LINE_TOKEN}"
}
LINE_REPLY_URL = "https://api.line.me/v2/bot/message/reply"


from apscheduler.schedulers.background import BackgroundScheduler
import atexit

# â”€â”€ APScheduler è¨»å†Šï¼šæ¯é€±å…­ 09:00 America/Vancouver è§¸ç™¼ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
_sq_scheduler = None
def _ensure_scheduler_for_sq_weekly():
    """
    ä»¥èƒŒæ™¯æ’ç¨‹æ–¹å¼ï¼Œå›ºå®šæ¯é€±å…­ 09:00ï¼ˆAmerica/Vancouverï¼‰åŸ·è¡Œ
    push_sq_weekly_shipments(force=False)ã€‚
    """
    global _sq_scheduler
    if _sq_scheduler is not None:
        return _sq_scheduler

    tz = pytz.timezone(TIMEZONE)
    sched = BackgroundScheduler(timezone=tz)
    sched.add_job(
        push_sq_weekly_shipments,
        trigger="cron",
        day_of_week="sat",
        hour=9,
        minute=0,
        kwargs={"force": False},
        id="sq_weekly_shipments_sat_9am",
        replace_existing=True,
        misfire_grace_time=600,
        coalesce=True,
        max_instances=1,
    )
    sched.start()
    atexit.register(lambda: sched.shutdown(wait=False))
    log.info("[SQ Weekly] Scheduler started (Sat 09:00 America/Vancouver).")

    _sq_scheduler = sched
    return _sq_scheduler

# --- Debug: print SA client_email once on startup (safe) ---
try:
    import os, json, base64
    sa_json = None
    if os.getenv("GCP_SA_JSON_BASE64"):
        sa_json = base64.b64decode(os.getenv("GCP_SA_JSON_BASE64")).decode("utf-8", "ignore")
    elif os.getenv("GOOGLE_SVCKEY_JSON"):
        sa_json = os.getenv("GOOGLE_SVCKEY_JSON")
    if sa_json:
        client_email = json.loads(sa_json).get("client_email")
        if client_email:
            print(f"[GSHEET] service account email = {client_email}")
except Exception as _e:
    print("[GSHEET] could not print service account email:", _e)

# â”€â”€â”€ Structured Logging Setup â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s %(levelname)s %(message)s"
)
log = logging.getLogger(__name__)

# â”€â”€â”€ Customer Mapping â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Map each LINE group to the list of lowercase keywords you filter on
CUSTOMER_FILTERS = {
    os.getenv("LINE_GROUP_ID_YUMI"):   ["yumi", "shu-yen"],
    os.getenv("LINE_GROUP_ID_VICKY"):  ["vicky","chia-chi"]
}

# æ¨¡çµ„è¼‰å…¥æ™‚å°±ç¢ºä¿ SQ æ’ç¨‹å•Ÿå‹•ï¼ˆèˆ‡ ACE çš„ _ensure_scheduler_for_ace_today ä¸¦å­˜ï¼‰
try:
    _ensure_scheduler_for_sq_weekly()
except Exception as _e:
    log.error(f"[SQ Weekly] Scheduler init failed: {_e}")

# â”€â”€ APScheduler è¨»å†Šï¼šæ¯é€±å››ï¼†é€±æ—¥ 16:00 America/Vancouver è§¸ç™¼ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
_scheduler = None
def _ensure_scheduler_for_ace_today():
    """
    ä»¥èƒŒæ™¯æ’ç¨‹æ–¹å¼ï¼Œå›ºå®šåœ¨æ¯é€±å››èˆ‡é€±æ—¥çš„ 16:00ï¼ˆAmerica/Vancouverï¼‰åŸ·è¡Œ
    push_ace_today_shipments(force=False)ã€‚
    - ä½¿ç”¨ coalesce / max_instances ä¾†é¿å…é‡å•Ÿé€ æˆçš„å †ç–Šè§¸ç™¼
    - ä½¿ç”¨ misfire_grace_time å…è¨±çŸ­æš«å–šé†’å»¶é²
    """
    global _scheduler
    if _scheduler is not None:
        return _scheduler

    tz = pytz.timezone(TIMEZONE)
    sched = BackgroundScheduler(timezone=tz)
    sched.add_job(
        push_ace_today_shipments,
        trigger="cron",
        day_of_week="thu,sun",
        hour=16,
        minute=0,
        kwargs={"force": False},  # æ’ç¨‹å‘¼å«ï¼Œä¸€å¾‹é force
        id="ace_today_shipments_thu_sun_4pm",
        replace_existing=True,
        misfire_grace_time=600,
        coalesce=True,
        max_instances=1,
    )
    sched.start()
    atexit.register(lambda: sched.shutdown(wait=False))
    log.info("[ACE Today] Scheduler started (Thu/Sun 16:00 America/Vancouver).")

    _scheduler = sched
    return _scheduler

# æ¨¡çµ„åŒ¯å…¥æ™‚å°±ç¢ºä¿æ’ç¨‹å•Ÿå‹•ï¼ˆå¤šæ¬¡åŒ¯å…¥ä¹Ÿå®‰å…¨ï¼‰
try:
    _ensure_scheduler_for_ace_today()
except Exception as _e:
    log.error(f"[ACE Today] Scheduler init failed: {_e}")

# â”€â”€â”€ ADDED: Configure OpenAI API key â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
openai.api_key = os.getenv("OPENAI_API_KEY")
OPENAI_MODEL = os.getenv("OPENAI_MODEL", "gpt-4o-mini")

# keep an in-memory buffer of successfully updated tracking IDs per group
_pending = defaultdict(list)
_scheduled = set()

def strip_mention(line):
    # Remove an @mention at the very start of the line (e.g. "@Gorsky ")
    return re.sub(r"^@\S+\s*", "", line)

def _schedule_summary(group_id):
    """Called once per 30m window to send the summary and clear the buffer."""
    ids = _pending.pop(group_id, [])
    _scheduled.discard(group_id)
    if not ids:
        return
    # dedupe and format
    uniq = sorted(set(ids))
    text = "âœ… Updated packages:\n" + "\n".join(f"- {tid}" for tid in uniq)
    payload = {
        "to": group_id,
        "messages": [{"type": "text", "text": text}]
    }
    requests.post(LINE_PUSH_URL, headers=LINE_HEADERS, json=payload)

MONDAY_API_URL    = "https://api.monday.com/v2"
MONDAY_TOKEN      = os.getenv("MONDAY_TOKEN")
VICKY_SUBITEM_BOARD_ID = 4815120249    # è«‹å¡«ä½  Vicky å­ä»»å‹™æ‰€åœ¨çš„ Board ID
VICKY_STATUS_COLUMN_ID = "status__1"   # è«‹å¡«æº«å“¥è¯æ”¶æ¬¾é‚£å€‹æ¬„ä½çš„ column_id



# def vicky_sheet_recently_edited():
    ##1) build a credentials object from your SERVICE_ACCOUNT JSON
    # creds = Credentials.from_service_account_info(
        # json.loads(os.environ["GOOGLE_SVCKEY_JSON"]),
        # scopes=SCOPES
    # )

    ##2) fetch the spreadsheetâ€™s Drive metadata
    # drive = build("drive", "v3", credentials=creds)
    # sheet_url = os.environ["VICKY_SHEET_URL"]
    # file_id = sheet_url.split("/")[5]            # extract the ID from the URL
    # meta = drive.files().get(
        # fileId=file_id,
        # fields="modifiedTime"
    # ).execute()

    ##3) parse the ISO timestamp into a datetime
    # last_edit = datetime.fromisoformat(meta["modifiedTime"].replace("Z","+00:00"))

    ##4) compare against now (UTC)
    # age = datetime.now(timezone.utc) - last_edit
    # return age.days < 3
  


# â”€â”€â”€ UPS tracking normalization â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def normalize_ups(trk: str) -> str:
    s = re.sub(r'[^A-Za-z0-9]', '', trk or '').upper()
    if s.startswith('1Z'):
        head, tail = s[:2], s[2:]
        tail = tail.replace('O', '0')  # OCR fix: Oâ†’0 after 1Z
        s = head + tail
    return s

# â”€â”€â”€ lookup_full_tracking å®šç¾© â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def lookup_full_tracking(ups_last4: str) -> Optional[str]:
    """
    åœ¨ Tracking å·¥ä½œè¡¨çš„ S/T/U æ¬„æ‰¾å”¯ä¸€å°¾è™ŸåŒ¹é…ï¼Œå›å‚³å®Œæ•´è¿½è¹¤ç¢¼æˆ– Noneã€‚
    """
    SHEET_ID = "1BgmCA1DSotteYMZgAvYKiTRWEAfhoh7zK9oPaTTyt9Q"
    gs = get_gspread_client()
    ss = gs.open_by_key(SHEET_ID)
    ws = ss.worksheet("Tracking")

    cols = [19, 20, 21]  # S=19, T=20, U=21
    matches = []
    for col_idx in cols:
        vals = ws.col_values(col_idx)
        for v in vals[1:]:
            v = (v or "").strip()
            if len(v) >= 4 and v[-4:] == ups_last4:
                matches.append(v)

    if len(matches) != 1:
        log.warning(f"UPSå°¾è™Ÿ {ups_last4} æ‰¾åˆ° {len(matches)} ç­†ï¼Œä¸å”¯ä¸€ï¼Œè·³é")
        return None
    return matches[0]

def _line_push(target_id, text):
    """é€šç”¨ LINE PUSH å‡½å¼"""
    payload = {
        "to": target_id,
        "messages": [{"type": "text", "text": text}]
    }
    resp = requests.post(LINE_PUSH_URL, headers=LINE_HEADERS, json=payload)
    log.info(f"[_line_push] to {target_id}: {resp.status_code}")
    return resp

# CLI entrypoint
def main():
    pdf_path = "U110252577.pdf"
    dpi = 300
    prompt = OCR_SHIPPING_PROMPT

    # Convert and extract
    images = pdf_to_image(pdf_path, dpi=dpi)
    text = extract_text_from_images(images, prompt=prompt)
    print(text)

# â”€â”€â”€ Flask Webhook â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
app = Flask(__name__)

ocr_helper = OCRAgent()

CONFIG = {
    'VICKY_GROUP_ID': VICKY_GROUP_ID,
    'YUMI_GROUP_ID': YUMI_GROUP_ID,
    'IRIS_GROUP_ID': IRIS_GROUP_ID,
    'YVES_USER_ID': YVES_USER_ID,
    'GORSKY_USER_ID': GORSKY_USER_ID,
    'VICKY_NAMES': config.VICKY_NAMES,
    'YUMI_NAMES': config.YUMI_NAMES,
    'IRIS_NAMES': config.IRIS_NAMES,
    'YVES_NAMES': config.YVES_NAMES,
    'CODE_TRIGGER_RE': CODE_TRIGGER_RE,
    'ACE_SHEET_URL': ACE_SHEET_URL
}

shipment_parser = ShipmentParserService(CONFIG, get_gspread_client, _line_push)

monday_service = MondaySyncService(
    api_token=MONDAY_API_TOKEN,
    gspread_client_func=get_gspread_client,
    line_push_func=_line_push
)

@app.route("/webhook", methods=["GET", "POST"])
def webhook():

    import re
    # Log incoming methods
    # print(f"[Webhook] Received {request.method} to /webhook")
    # log.info(f"Received {request.method} to /webhook")
    if request.method == "GET":
        return "OK", 200

    data = request.get_json()
    print("[Webhook] Payload:", json.dumps(data, ensure_ascii=False))
    # log.info(f"Payload: {json.dumps(data, ensure_ascii=False)}")

    for event in data.get("events", []):
        # ignore nonâ€message events (eg. unsend)
        if event.get("type") != "message":
            continue
            
        # ç«‹åˆ»æŠ“ source / group_id
        src = event["source"]
        group_id = src.get("groupId")
        msg      = event["message"]
        text     = msg.get("text", "").strip()
        mtype    = msg.get("type")
    
        # â”€â”€â”€ NEW & CLEANED PDF OCR Trigger â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        if (
            msg.get("type") == "file"
            and msg.get("fileName", "").lower().endswith(".pdf")
            and src.get("groupId") in {VICKY_GROUP_ID, YUMI_GROUP_ID, JOYCE_GROUP_ID, IRIS_GROUP_ID, PDF_GROUP_ID}
        ):
            file_id = msg["id"]
            original_filename = msg.get("fileName", "uploaded.pdf")
            
            try:
                # 1) Download the PDF bytes from LINE
                resp = requests.get(
                    f"https://api-data.line.me/v2/bot/message/{file_id}/content",
                    headers={"Authorization": f"Bearer {LINE_TOKEN}"},
                )
                resp.raise_for_status()
                pdf_bytes = resp.content

                # 2) Use the isolated OCR Engine
                # This calls the class we created in ocr_engine.py
                full_data = ocr_helper.process_shipment_pdf(pdf_bytes)

                if not full_data:
                    log.error("[PDF OCR] Engine returned no data")
                    return "OK", 200

                log.info(f"[PDF OCR] Extracted data: {full_data}")

                # 3) Run Monday.com sync in a background thread to prevent timeouts
                import threading
                threading.Thread(
                    target=monday_service.run_sync,
                    args=(full_data, pdf_bytes, original_filename, r, group_id),
                    daemon=True
                ).start()

            except Exception as e:
                log.error(f"[PDF OCR] Critical failure: {e}", exc_info=True)
                _line_push(YVES_USER_ID, f"âš ï¸ PDF System Error: {str(e)}")

            return "OK", 200

        # ğŸŸ¢ æ–°å¢ï¼šåœ–ç‰‡æ¢ç¢¼è¾¨è­˜é‚è¼¯
        if mtype == "image":
            # å‘¼å« barcode_service è™•ç†ï¼Œå‚³å…¥æ‰€éœ€çš„ç·©å­˜èˆ‡å›å‘¼å‡½å¼
            if handle_barcode_image(event, group_id, r, _pending, _scheduled, _schedule_summary):
                continue # å¦‚æœè™•ç†æˆåŠŸï¼ˆæ˜¯æ¢ç¢¼åœ–ç‰‡ï¼‰ï¼Œå‰‡è·³éå¾ŒçºŒé‚è¼¯

        # ğŸŸ¢ NEW: TWWS å…©æ®µå¼äº’å‹•é‚è¼¯ (é™å®šå€‹äººç§è¨Šä¸”é™å®š Yves ä½¿ç”¨)
        user_id = src.get("userId")
        twws_state_key = f"twws_wait_{user_id}" # ä½¿ç”¨ userId ç¢ºä¿ç‹€æ…‹å”¯ä¸€
        
        # æª¢æŸ¥æ˜¯å¦ç‚ºã€Œå€‹äººç§è¨Šã€ä¸”ç‚ºã€ŒæŒ‡å®šçš„ç®¡ç†å“¡ (Yves)ã€
        if src.get("type") == "user" and user_id == YVES_USER_ID:
            # æª¢æŸ¥æ˜¯å¦æ­£åœ¨ç­‰å¾…ä½¿ç”¨è€…è¼¸å…¥ã€Œå­é …ç›®åç¨±ã€
            if r.get(twws_state_key):
                # å¦‚æœæœ‰ç‹€æ…‹å­˜åœ¨ï¼ŒæŠŠé€™æ¬¡è¼¸å…¥çš„ text ç•¶ä½œåç¨±å»æŸ¥
                amount = get_twws_value_by_name(text)
                # ä½¿ç”¨ user_id ä½œç‚ºæ¨æ’­å°è±¡ï¼Œç¢ºä¿ç§è¨Šå›å‚³
                _line_push(user_id, f"ğŸ” æŸ¥è©¢çµæœ ({text}):\nğŸ’° æ‡‰ä»˜é‡‘é¡: {amount}")
                r.delete(twws_state_key) # æŸ¥å®Œå¾Œåˆªé™¤ç‹€æ…‹
                continue

            # è§¸ç™¼ç¬¬ä¸€éšæ®µï¼šä½¿ç”¨è€…è¼¸å…¥ twws
            if text.lower() == "twws":
                # è¨­å®šç‹€æ…‹ä¸¦çµ¦äºˆ 5 åˆ†é˜ (300ç§’) çš„æ™‚é™
                r.set(twws_state_key, "active", ex=300)
                _line_push(user_id, "å¥½çš„ï¼Œè«‹è¼¸å…¥å­é …ç›®åç¨±ï¼š")
                continue

            # ğŸŸ¢ æ–°å¢ï¼šæœªä»˜æ¬¾é …æŸ¥è©¢æŒ‡ä»¤
            if text.lower().startswith("unpaid"):
                handle_unpaid_event(
                    sender_id=group_id if group_id else user_id,
                    message_text=text,
                    reply_token=event["replyToken"],
                    user_id=user_id,
                    group_id=group_id
                )
                continue

        # --- é‡‘é¡è‡ªå‹•éŒ„å…¥é‚è¼¯ï¼šåƒ…é™ PDF Scanning ç¾¤çµ„è§¸ç™¼ ---
        if group_id == PDF_GROUP_ID:
            # æª¢æŸ¥æ˜¯å¦ç‚ºç´”æ•¸å­—é‡‘é¡ (å¦‚ 43.10)
            if re.match(r'^\d+(\.\d{1,2})?$', text):
                # å¾å…¨å±€ Key æŠ“å–æœ€å¾Œä¸€æ¬¡ä¸Šå‚³çš„ PDF é …ç›® ID, å–å¾—åŒ…å« ID èˆ‡ Board çš„çµ„åˆå­—ä¸²
                redis_val = r.get("global_last_pdf_parent")

                if redis_val and "|" in redis_val:
                    # æ‹†åˆ†å‡ºé …ç›® ID èˆ‡æ¿å¡Š ID
                    last_pid, last_bid = redis_val.split("|")

                    # å‘¼å«æ™‚å¤šå‚³å…¥æ¿å¡Š ID
                    ok, msg, item_name = monday_service.update_domestic_expense(last_pid, text, group_id, last_bid)

                    if ok:
                        _line_push(group_id, f"âœ… å·²æˆåŠŸç™»è¨˜å¢ƒå…§æ”¯å‡º: ${text}\nğŸ“Œ é …ç›®: {item_name}")
                        r.delete("global_last_pdf_parent")
                    else:
                        _line_push(group_id, f"âŒ ç™»è¨˜å¤±æ•—: {msg}\nğŸ“Œ é …ç›®: {item_name if item_name else 'æœªçŸ¥'}")
                    continue
        
        # 1) å¤šç­† UPS æœ«å››ç¢¼ï¼‹é‡é‡ï¼‹å°ºå¯¸ ä¸€æ¬¡è™•ç†
        # åŒæ™‚æ”¯æ´ã€Œ*ã€ã€ŒÃ—ã€ã€Œxã€æˆ–ã€Œç©ºç™½ã€åˆ†éš”
        multi_pat = re.compile(
            r'(\d{4})\s+'             # 4ä½UPSå°¾è™Ÿ
            r'([\d.]+)kg\s+'          # é‡é‡ (kg)
            r'(\d+)'                  # å¯¬
            r'(?:[Ã—x*\s]+)'           # å…è¨± Ã— x * æˆ–ç©ºç™½ ä½œç‚ºåˆ†éš”
            r'(\d+)'                  # é«˜
            r'(?:[Ã—x*\s]+)'           # å†æ¬¡å…è¨±å„ç¨®åˆ†éš”
            r'(\d+)'                  # æ·±
            r'(?:cm)?',               # å¯é¸çš„ã€Œcmã€
            re.IGNORECASE
        )
        matches = multi_pat.findall(text)  # æ‰¾å‡ºæ‰€æœ‰ç¬¦åˆæ ¼å¼çš„ tuple åˆ—è¡¨

        if matches:
            for ups4, wt_str, w, h, d in matches:
                # â€”(1) å¾ Google Sheets æ‰¾å›å®Œæ•´è¿½è¹¤ç¢¼
                full_no = lookup_full_tracking(ups4)
                if not full_no:
                    # å¦‚æœæ‰¾ä¸åˆ°æˆ–ä¸å”¯ä¸€ï¼Œè·³éæœ¬ç­†
                    continue

                # â€”(2) è§£æé‡é‡èˆ‡å°ºå¯¸
                weight_kg = float(wt_str)      # å°‡å­—ä¸²è½‰ç‚º float
                dims_norm = f"{w}*{h}*{d}"    # çµ„æˆ "é•·*å¯¬*é«˜" å­—ä¸²

                # â€”(3) ç”¨å®Œæ•´è¿½è¹¤ç¢¼åˆ° Monday æŸ¥ subitem (Name æ¬„)
                find_q = f'''
                query {{
                  items_by_column_values(
                    board_id: {os.getenv("AIR_BOARD_ID")},
                    column_id: "name",
                    column_value: "{full_no}"
                  ) {{ id }}
                }}'''
                resp = requests.post(
                    "https://api.monday.com/v2",
                    headers={ "Authorization": MONDAY_API_TOKEN,
                              "Content-Type":  "application/json" },
                    json={ "query": find_q }
                )
                items = resp.json().get("data", {}) \
                                 .get("items_by_column_values", [])
                if not items:
                    log.warning(f"Monday: subitem åç¨±={full_no} æ‰¾ä¸åˆ°ï¼Œè·³é")
                    continue

                sub_id = items[0]["id"]  # å–ç¬¬ä¸€å€‹ match çš„ subitem ID

                # â€”(4) ä¸Šå‚³å°ºå¯¸ (__1__cm__1 æ¬„)
                dim_mut = f'''
                mutation {{
                  change_simple_column_value(
                    item_id: {sub_id},
                    board_id: {os.getenv("AIR_BOARD_ID")},
                    column_id: "__1__cm__1",
                    value: "{dims_norm}"
                  ) {{ id }}
                }}'''
                requests.post(
                    "https://api.monday.com/v2",
                    headers={ "Authorization": MONDAY_API_TOKEN,
                              "Content-Type":  "application/json" },
                    json={ "query": dim_mut }
                )

                # â€”(5) ä¸Šå‚³é‡é‡ (numeric__1 æ¬„)
                wt_mut = f'''
                mutation {{
                  change_simple_column_value(
                    item_id: {sub_id},
                    board_id: {os.getenv("AIR_BOARD_ID")},
                    column_id: "numeric__1",
                    value: "{weight_kg:.2f}"
                  ) {{ id }}
                }}'''
                requests.post(
                    "https://api.monday.com/v2",
                    headers={ "Authorization": MONDAY_API_TOKEN,
                              "Content-Type":  "application/json" },
                    json={ "query": wt_mut }
                )

                # â€”(6) ç¿»è½‰ç‹€æ…‹åˆ°ã€Œæº«å“¥è¯æ”¶æ¬¾ã€(status__1 æ¬„)
                stat_mut = f'''
                mutation {{
                  change_simple_column_value(
                    item_id: {sub_id},
                    board_id: {os.getenv("AIR_BOARD_ID")},
                    column_id: "status__1",
                    value: "{{\\"label\\":\\"æº«å“¥è¯æ”¶æ¬¾\\"}}"
                  ) {{ id }}
                }}'''
                requests.post(
                    "https://api.monday.com/v2",
                    headers={ "Authorization": MONDAY_API_TOKEN,
                              "Content-Type":  "application/json" },
                    json={ "query": stat_mut }
                )

                # â€”(7) æ—¥èªŒï¼šç¢ºèªæ›´æ–°å®Œç•¢
                log.info(f"[UPSâ†’Monday] {full_no} æ›´æ–°: é‡é‡={weight_kg}kg, å°ºå¯¸={dims_norm}")

            # è™•ç†å®Œæ‰€æœ‰å¤šç­† UPS å¾Œï¼Œè·³éå¾ŒçºŒä»»ä½• handler
            continue

        # 2) pending_key å–®ç­† size/weight parser
        pending_key = f"last_subitem_for_{group_id}"
        sub_id = r.get(pending_key)
        if sub_id:
            size_text = text
            log.info(f"Parsing size_text for subitem {sub_id!r}: {size_text!r}")

            # parse weight
            wm = re.search(r"(\d+(?:\.\d+)?)\s*(kg|å…¬æ–¤|lbs?)", size_text, re.IGNORECASE)
            if wm:
                qty, unit = float(wm.group(1)), wm.group(2).lower()
                weight_kg = qty * (0.453592 if unit.startswith("lb") else 1.0)
                log.info(f"  â†’ Parsed weight_kg: {weight_kg:.2f} kg")
            else:
                weight_kg = None

            # parse dimensions
            dm = re.search(
              # allow Ã—, x, *, or any whitespace between numbers
              r"(\d+(?:\.\d+)?)[Ã—x*\s]+(\d+(?:\.\d+)?)[Ã—x*\s]+(\d+(?:\.\d+)?)(?:\s*)(cm|å…¬åˆ†|in|å‹)?",
              size_text, re.IGNORECASE
            )
            if dm:
                # capture groups: 1=width, 2=height, 3=depth, 4=unit (optional)
                w, h, d = map(float, dm.group(1,2,3))
                unit = (dm.group(4) or "cm").lower()
                factor = 2.54 if unit.startswith(("in","å‹")) else 1.0
                # use '*' between numbers, always
                dims_norm = f"{int(w*factor)}*{int(h*factor)}*{int(d*factor)}"
                log.info(f"  â†’ Parsed dims_norm: {dims_norm}")
            else:
                dims_norm = None
                log.debug("  â†’ No dimensions match")

            # helper to build the mutation
            def mutate(colId, val):
                return f'''
                mutation {{
                  change_simple_column_value(
                    item_id: {sub_id},
                    board_id: {os.getenv("AIR_BOARD_ID")},
                    column_id: "{colId}",
                    value: "{val}"
                  ) {{ id }}
                }}'''

            # push dimensions if found
            if dims_norm:
                requests.post(
                  "https://api.monday.com/v2",
                  headers={ "Authorization": MONDAY_API_TOKEN, "Content-Type": "application/json" },
                  json={ "query": mutate("__1__cm__1", dims_norm) }
                )

            # push weight if found
            if weight_kg is not None:
                requests.post(
                  "https://api.monday.com/v2",
                  headers={ "Authorization": MONDAY_API_TOKEN, "Content-Type": "application/json" },
                  json={ "query": mutate("numeric__1", f"{weight_kg:.2f}") }
                )
                
            # now that we got weight, clear pending so we don't parse again
            r.delete(pending_key)
            log.info(f"Cleared pending for subitem {sub_id}")

            # â”€â”€ if dims+weight and status is â€œæ¸¬é‡â€, bump to â€œæº«å“¥è¯æ”¶æ¬¾â€ â”€â”€â”€â”€â”€
            if dims_norm is not None and weight_kg is not None:
                status_mut = f'''
                mutation {{
                  change_column_value(
                    item_id: {sub_id},
                    board_id: {os.getenv("AIR_BOARD_ID")},
                    column_id: "status__1",
                    value: "{{\\"label\\":\\"æº«å“¥è¯æ”¶æ¬¾\\"}}"
                  ) {{ id }}
                }}'''
                resp = requests.post(
                  "https://api.monday.com/v2",
                  headers={
                    "Authorization": MONDAY_API_TOKEN,
                    "Content-Type":  "application/json"
                  },
                  json={ "query": status_mut }
                )
                if resp.status_code == 200:
                    log.info(f"Updated status to æº«å“¥è¯æ”¶æ¬¾ for subitem {sub_id}")
                else:
                    log.error(f"Failed to update status for subitem {sub_id}: {resp.text}")

            # whether dims or weight or both, log final
            log.info(f"Finished size/weight sync for subitem {sub_id}: dims={dims_norm!r}, weight={weight_kg!r}")
            continue
 
        # 3) Ace schedule (é€±å››ï¼é€±æ—¥å‡ºè²¨) & ACE EZ-Way check
        if group_id == ACE_GROUP_ID and ("é€±å››å‡ºè²¨" in text or "é€±æ—¥å‡ºè²¨" in text):
            handle_ace_schedule(event)
            handle_ace_ezway_check_and_push_to_yves(event)
            continue

        # 4) è™•ç†ã€Œç”³å ±ç›¸ç¬¦ã€æé†’
        if "ç”³å ±ç›¸ç¬¦" in text and CODE_TRIGGER_RE.search(text):
            handle_missing_confirm(event)
            continue
        
        # 5) Richmond-arrival triggers content-request to Vicky â€”â€”â€”â€”â€”â€”â€”â€”â€”
        if group_id == VICKY_GROUP_ID and "[Richmond, Canada] å·²åˆ°é”æ´¾é€ä¸­å¿ƒ" in text:
            # extract the tracking ID inside parentheses
            import re
            m = re.search(r"\(([^)]+)\)", text)
            if m:
                tracking_id = m.group(1)
            else:
                # no ID found, skip
                continue

            # build the mention message
            placeholder = "{user1}"
            msg = f"{placeholder} è«‹æä¾›æ­¤åŒ…è£¹çš„å…§å®¹ç‰©æ¸…å–®ï¼š{tracking_id}"
            substitution = {
                "user1": {
                    "type": "mention",
                    "mentionee": {
                        "type":   "user",
                        "userId": VICKY_USER_ID
                    }
                }
            }
            payload = {
                "to": VICKY_GROUP_ID,
                "messages": [{
                    "type":        "textV2",
                    "text":        msg,
                    "substitution": substitution
                }]
            }
            requests.post(LINE_PUSH_URL, headers=LINE_HEADERS, json=payload)
            log.info(f"Requested contents list from Vicky for {tracking_id}")
            continue
                
        # 6) Soquick â€œä¸Šå‘¨å…­å‡ºè²¨åŒ…è£¹çš„æ´¾ä»¶å–®è™Ÿâ€ & Ace "å‡ºè²¨å–®è™Ÿ" blocks â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”
        if (group_id == SOQUICK_GROUP_ID and "ä¸Šå‘¨å…­å‡ºè²¨åŒ…è£¹çš„æ´¾ä»¶å–®è™Ÿ" in text) or (group_id == ACE_GROUP_ID and "å‡ºè²¨å–®è™Ÿ" in text and "å®…é…å–®è™Ÿ" in text):
            handle_soquick_and_ace_shipments(event)
            continue

        # 7) Soquick â€œè«‹é€šçŸ¥â€¦ç”³å ±ç›¸ç¬¦â€ messages â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”
        log.info(
            "[SOQ DEBUG] group_id=%r, SOQUICK_GROUP_ID=%r, "
            "has_æ‚¨å¥½=%r, has_æŒ‰=%r, has_ç”³å ±ç›¸ç¬¦=%r",
            group_id,
            SOQUICK_GROUP_ID,
            "æ‚¨å¥½ï¼Œè«‹" in text,
            "æŒ‰" in text,
            "ç”³å ±ç›¸ç¬¦" in text,
        )        
        if (group_id == SOQUICK_GROUP_ID
            and "æ‚¨å¥½ï¼Œè«‹" in text
            and "æŒ‰" in text
            and "ç”³å ±ç›¸ç¬¦" in text):
            shipment_parser.handle_soquick_full_notification(event)
            continue          

        # 8) Your existing â€œè¿½è¹¤åŒ…è£¹â€ logic
        if text == "è¿½è¹¤åŒ…è£¹":
            keywords = CUSTOMER_FILTERS.get(group_id)
            if not keywords:
                print(f"[Webhook] No keywords configured for group {group_id}, skipping.")
                continue

            # Now safe to extract reply_token
            reply_token = event["replyToken"]
            print("[Webhook] Trigger matched, fetching statusesâ€¦")
            messages = get_statuses_for(keywords)
            print("[Webhook] Reply messages:", messages)

            # Combine lines into one multi-line text
            combined = "\n\n".join(messages)
            payload = {
                "replyToken": reply_token,
                "messages": [{"type": "text", "text": combined}]
            }

            headers = {
                "Content-Type": "application/json",
                "Authorization": f"Bearer {LINE_TOKEN}"
            }
            resp = requests.post(
                "https://api.line.me/v2/bot/message/reply",
                headers=headers,
                json=payload
            )
            print(f"[Webhook] LINE reply status: {resp.status_code}, body: {resp.text}")
            log.info(f"LINE reply status={resp.status_code}, body={resp.text}")

        # 9) Your existing â€œä¸‹å€‹åœ‹å®šå‡æ—¥â€ logic
        if text == "ä¸‹å€‹åœ‹å®šå‡æ—¥":
            from holiday_reminder import get_next_holiday
            msg = get_next_holiday()
            reply_token = event["replyToken"]
            requests.post(
                "https://api.line.me/v2/bot/message/reply",
                headers={
                    "Authorization": f"Bearer {LINE_TOKEN}",
                    "Content-Type": "application/json"
                },
                json={
                    "replyToken": reply_token,
                    "messages": [{"type": "text", "text": msg}]
                }
            )

        # ğŸŸ¢ NEW: ACE manual trigger â€œå·²ä¸Šå‚³è³‡æ–™å¯å‡ºè²¨â€
        if (
            event.get("source", {}).get("type") == "group"
            and event["source"].get("groupId") == ACE_GROUP_ID
            and text.strip() == "å·²ä¸Šå‚³è³‡æ–™å¯å‡ºè²¨"
        ):
            reply_token = event.get("replyToken")
            push_ace_today_shipments(force=True, reply_token=reply_token)
            return "OK", 200

    return "OK", 200
    
# â”€â”€â”€ Monday.com Webhook â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
@app.route("/monday-webhook", methods=["GET", "POST"])
def monday_webhook():
    if request.method == "GET":
        return "OK", 200

    data = request.get_json()
    evt  = data.get("event", data)
    # respond to Mondayâ€™s handshake
    if "challenge" in data:
        return jsonify({"challenge": data["challenge"]}), 200

    sub_id    = evt.get("pulseId") or evt.get("itemId")
    parent_id = evt.get("parentItemId")
    lookup_id = parent_id or sub_id
    new_txt   = evt.get("value", {}).get("label", {}).get("text")

    # only act when Location flips to åœ‹éš›é‹è¼¸
    if new_txt != "åœ‹éš›é‹è¼¸" or not lookup_id:
        return "OK", 200

    # fetch just the formula column:
    gql = '''
    query ($itemIds: [ID!]!) {
      items(ids: $itemIds) {
        column_values(ids: ["formula8__1"]) {
          id
          text
          ... on FormulaValue { display_value }
        }
      }
    }'''
    variables = {"itemIds": [str(lookup_id)]}
    resp = requests.post(
      "https://api.monday.com/v2",
      json={"query": gql, "variables": variables},
      headers={
        "Authorization": MONDAY_API_TOKEN,
        "Content-Type":  "application/json"
      }
    )
    data2 = resp.json()

    # grab that single column_value
    cv = data2["data"]["items"][0]["column_values"][0]
    client = (cv.get("text") or cv.get("display_value") or "").strip()
    key    = client.lower()     # e.g. "yumi" or "vicky"

    group_id = CLIENT_TO_GROUP.get(key)
    if not group_id:
        print(f"[Mondayâ†’LINE] no mapping for â€œ{client}â€ â†’ {key}, skipping.")
        log.warning(f"No mapping for client={client} key={key}, skipping.")
        return "OK", 200

    item_name = evt.get("pulseName") or str(lookup_id)
    message   = f"ğŸ“¦ {item_name} å·²é€å¾€æ©Ÿå ´ï¼Œæº–å‚™é€²è¡Œåœ‹éš›é‹è¼¸ã€‚"

    push = requests.post(
      "https://api.line.me/v2/bot/message/push",
      headers={
        "Authorization": f"Bearer {LINE_TOKEN}",
        "Content-Type":  "application/json"
      },
      json={"to": group_id, "messages":[{"type":"text","text":message}]}
    )
    print(f"[Mondayâ†’LINE] sent to {client}: {push.status_code}", push.text)
    log.info(f"Mondayâ†’LINE push status={push.status_code}, body={push.text}")

    return "OK", 200
 
# â”€â”€â”€ Poller State Helpers & Job â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# â”€â”€â”€ Helpers for parsing batch lines â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

##â€”â€”â€” Vicky reminders (Wed & Fri at 18:00) â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”
# sched.add_job(lambda: remind_vicky("æ˜ŸæœŸå››"),
              # trigger="cron", day_of_week="wed", hour=18, minute=00)
# sched.add_job(lambda: remind_vicky("é€±æœ«"),
              # trigger="cron", day_of_week="fri", hour=17, minute=00)

# sched.start()
# log.info("Scheduler started")

if __name__ == "__main__":
    app.run(host="0.0.0.0", port=int(os.getenv("PORT", 5000)))